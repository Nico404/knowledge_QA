{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Knowledge-base QA using chroma DB, langchain and mistral LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import a bunch of PDF, chunk them up and store their embeddings in a database. Then, prompt the database with a question and retrieve the most relevant documents through language chaining and mistral LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='sjdfbwdf wfgjkergjher wgerg ewrg ergg ergewrwgerrg r erg gewrg erwg reg wgerg erg erge\\\\n erg wreg \\\\n', metadata={'source': 'documents/txt/toto.txt'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading txt documents for testing purposes\n",
    "txt_directory = 'documents/txt/'\n",
    "\n",
    "def load_txt_docs(directory):\n",
    "  loader = DirectoryLoader(directory)\n",
    "  documents = loader.load()\n",
    "  return documents\n",
    "\n",
    "txt_documents = load_txt_docs(txt_directory)\n",
    "txt_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='SUPERVISED CHORUS DETECTION FOR POPULAR MUSIC USING CONVOLUTIONAL\\nNEURAL NETWORK AND MULTI-TASK LEARNING\\nJu-Chiang Wang, Jordan B.L. Smith, Jitong Chen, Xuchen Song, and Yuxuan Wang\\nByteDance\\n{ju-chiang.wang, jordan.smith, chenjitong.1, xuchen.song, wangyuxuan.11 }@bytedance.com\\nABSTRACT\\nThis paper presents a novel supervised approach to de-\\ntecting the chorus segments in popular music. Traditional ap-\\nproaches to this task are mostly unsupervised, with pipelines\\ndesigned to target some quality that is assumed to deﬁne\\n“chorusness,” which usually means seeking the loudest or\\nmost frequently repeated sections. We propose to use a\\nconvolutional neural network with a multi-task learning ob-\\njective, which simultaneously ﬁts two temporal activation\\ncurves: one indicating “chorusness” as a function of time,\\nand the other the location of the boundaries. We also propose\\na post-processing method that jointly takes into account the\\nchorus and boundary predictions to produce binary output.\\nIn experiments using three datasets, we compare our system\\nto a set of public implementations of other segmentation and\\nchorus-detection algorithms, and ﬁnd our approach performs\\nsigniﬁcantly better.\\nIndex Terms —Chorus detection, CNN, multi-task learn-\\ning, music structural segmentation.\\n1. INTRODUCTION\\nVerse-chorus song form is a very common structure for popu-\\nlar music. In it, verses alternate with choruses, with the lyrics\\nof the verses varying and the choruses repeating more strictly\\nand more frequently. The authors of [1] cite other generaliza-\\ntions used to deﬁne choruses, including that they are the ‘most\\nprominent’ and ‘most catchy’ sections of a piece. These traits\\nmake it desirable to detect choruses automatically, whether\\nfor generating “thumbnails” [2, 3, 4], for ﬁnding the emo-\\ntional “highlights” of a piece [5], or for enabling convenient\\nnavigation based on the song structure [6].\\nHowever, most previous approaches to chorus detection\\nand thumbnailing [7, 8, 2, 3] are unsupervised. They be-\\ngin with an observation about what typeﬁes chorus sections,\\nand search for them on this basis: e.g., ﬁnding the loudest,\\nmost frequently repeated, and/or the most homogenous sec-\\ntion. Since the deﬁnition of ‘chorus’ is a generalization that\\ndoes not apply in all cases, even a perfectly-designed system\\nof this type will fail to detect the chorus in many songs. A bet-\\nter approach may be to let a model learn what deﬁnes ‘cho-rusness’ from labeled examples; this would allow a system to\\nleverage the timbral and spectral features identiﬁed by [1] in\\na study of what acoustic features differentiate choruses.\\nThis approach, when applied to the related task of music\\nboundary detection by [9], led to a huge leap in the state of the\\nart. Prior segmentation algorithms would generally focus on a\\ndeﬁnable proxy task (e.g., detecting points of change or onsets\\nof repetitions), assisted by sensible heuristics (e.g., rounding\\nboundary estimates to the nearest downbeat). A convolutional\\nneural network (CNN) is trained to detect whether the cen-\\nter of a 16-second input is a boundary. When post-processed\\nwith an appropriate threshold, [9] demonstrated a 10% im-\\nprovement in f-measure over the state of the art.\\nWe propose a similar approach: train a neural network to\\npredict the “chorusness” of an excerpt directly from the au-\\ndio, and without the context of the rest of the song. We train a\\nbinary classiﬁer to predict the “chorusness” of each point in a\\nwindow, and slide this window throughout the song to obtain\\na chorus probability curve. However, this leaves the problem\\nof ﬁnding an appropriate threshold for post-processing. To\\nease this, we propose to jointly model the chorus activation\\nand boundary activation curves, so that the loss on the signals\\naround the boundaries is naturally emphasized. At the infer-\\nence phase, it also eases the process of converting the raw\\nprobability curve to a binary output for a song.\\nChorus detection is clearly related to two tasks with a long\\ntradition of MIR research: thumbnailing and music structure\\nanalysis (MSA) [10]. The objective of thumbnailing is to ﬁnd\\na short excerpt of a song that would be an effective preview.\\nHowever, there is no deﬁnition of what makes a good preview;\\n[3] cited several. In practice, thumbnailing systems are evalu-\\nated by testing how often they select all or part of a chorus [2],\\nor whichever segment is repeated most often [4]. Recently,\\n[5] proposed a novel, related objective—to ﬁnd the emotional\\nhighlights of pop songs—and evaluated their system based\\non whether it captured the choruses, which were assumed to\\ncorrespond to the highlights, but their system used a neural\\nnetwork trained to detect emotion, not choruses.\\nIn music structure analysis, it is assumed that one family\\nof segments corresponds to the chorus, but predicting which\\none is only rarely attempted. We are aware of three prior\\nsystems: [11], who assumed a highly restricted template forarXiv:2103.14253v2  [eess.AS]  21 Apr 2021', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='Fig. 1 . The system diagram.\\nsong structures and used heuristics to predict labels; [12], who\\npaired a standard structure analysis system with an HMM\\ntrained to label the sections; and [13], published very recently,\\nwho proposed a hierarchical generative model (with section\\nparts generating chord progressions, and these in turn gen-\\nerating observed feature sequences). This last model bene-\\nﬁts from supervision, but still relies on a hand-set strategy of\\ndetecting homogeneity and repetitions, based on handcrafted\\nfeatures (chroma and MFCCs).\\nThe lack of attention paid to chorus detection may be\\ndue to the difﬁculty of obtaining sufﬁcient training data.\\nSALAMI [14] contains 1446 songs, but these come from di-\\nverse genres, so it may be difﬁcult to learn a coherent notion\\nof “chorusness” from it. Introduced in 2019, the Harmonix\\nSet [15] contains 912 songs, 888 with “chorus” sections; it is\\nthe most frequent label, with over 3100 choruses altogether,\\nwhich is 41% more than the “verse” instances. We also have\\nthe annotated chorus locations for an internal dataset (denoted\\nasIn-House ) of 2480 Asian pop songs. We use these three\\nsources to train or evaluate our system. Since the data sources\\nall have different properties, we investigate the cross-dataset\\nperformance of our system.\\nIn addition to the usefulness of detecting choruses for\\nother applications, the annotations of choruses (that we de-\\npend on) seem more reliable than for other sections. In\\nSALAMI, we observed that if one annotator perceives a seg-\\nment starting at time t, there is a 66% chance that the other\\nannotator placed a boundary at the same time (within 0.5\\nseconds)—but this probability rises to 78% if the boundary\\nmarks the start of a ‘chorus’. This greater agreement could\\nbe the result of choruses having more salient beginnings than\\nother section types [16]. Therefore, the reliability of the\\nannotations makes a supervised system more feasible.\\n2. PROPOSED APPROACH\\nThis section details the three main stages of the system. The\\noverall pipeline is illustrated in Figure 1.2.1. Feature and Label Pre-processing\\nWe use the mel-spectrogram of a song as input. The model\\ntakes a window of Nframes (deﬁned as a chunk ) with a hop\\nsize ofSframes at a time. Note that Nis appropriately large\\nto allow the model to see longer contexts of the audio.\\nThe annotations include the starting and ending time-\\nstamps of each chorus. For each song, we create two types\\nof target labels: a chorus activation curve cand a bound-\\nary activation curve b. For a song of length L, we deﬁne\\nc= [c1,...,c L], withct= 1iftlies within a chorus section,\\nandct= 0 otherwise. To smooth the transitions, half of a\\n2-second wide Hann window is used to ramp from 0 to 1\\nprior to the chorus onset; a similar ramp down is added after\\nthe chorus offset. To create the boundary activation curve,\\nwe convert each boundary instant into a “boundary section”\\nof duration 0.5 seconds, and then apply the same ramp up\\nand down. Thus, each boundary produces a 2.5-second wide\\nbump in b. We use a wider target than in [9] to tolerate\\ngreater deviations from the true boundaries in our case, since\\nour goal is to predict the full extent of the chorus.\\nIn previous works [9, 17], the system models the prob-\\nability of a single target (i.e., a boundary) at the center of\\na chunk. By contrast, we design the system to model the\\nprobabilities of the entire activation curve in the chunk, with\\neach probability aligned with a frame in the mel-spectrogram.\\nThis enables the network to explicitly learn the contextual\\ndependency from the target activation curve. To sum up, a\\nchunk-level training sample for the CNN is represented as\\n{X∈RN×D,c∈RN,b∈RN}, where Xis the mel-\\nspectrogram using Dcomponents.\\n2.2. CNN-based Model\\nThe model is shown in the center part of Figure 1. To fa-\\ncilitate reproducibility, we adopt the model architecture pro-\\nposed in [18], which has shown excellent performance in mu-\\nsic classiﬁcation/tagging tasks. We make three modiﬁcations\\nto meet the requirements of our task: First, we add a tempo-\\nral max-pooling layer prior to the spectrum front-end model\\nto sub-sample the input mel-spectrogram. We use a pool size\\nof [6, 1] with a stride of [6, 1]. To ensure synchronization\\nwith the mel-spectrogram, we also apply median-pooling for\\ncandbwith a pool size of 6 with a stride of 6. Second, we\\nreplace the global pooling (for mean- and max-pooling over\\ntime) with a local pooling at the penultimate layer of the back-\\nend model. A pool size of [24, 1] and a stride of [12, 1] are\\nused. This design serves the need to model the entire temporal\\nactivation curve. Third, we add a ﬁnal dense layer to output\\nthe chorus and boundary predictions, denoted by ˆ c∈RN/6\\nandˆb∈RN/6, respectively. All the model parameters remain\\nthe same as [18] except those mentioned above.\\nTo achieve multi-task learning, we calculate the losses for\\nˆ candˆbseparately. Then, the ﬁnal loss is the weighted com-\\nbination:α·loss(ˆ c) + (1−α)·loss(ˆb), whereα∈[0,1]and', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='loss(·)is a reduce-mean operation that averages the element-\\nwise losses.\\n2.3. Output Merging and Post-processing\\nWe obtain the chunks from a song using a large overlap (e.g.\\n95%), so that during training, the model can see the labels\\nfor multiple times with multiple shifts of mel-spectrogram,\\nwhich is expected to help fast convergence. At the prediction\\nstage, we can merge the predictions of multiple overlapping\\nwindows to improve robustness. We take the average of the\\noverlapped probabilities to obtain the merged activation y[t]\\nat each global time step t∈[1,...,L ]of a song, which can\\nbe formulated as follow:\\ny[t] =1\\n|Q(t)|∑\\ni∈Q(t)ˆyi[m(i,t)], (1)\\nwhere{ˆyi[t′]},t′= [1,...,N ]is the predicted activation of\\nthei-th chunk,m(i,t)is the function that maps a global time\\nsteptto a local time step t′for the thei-th chunk, and the\\nfunctionQ(t)returns the set of chunks that are available at t.\\nFor example, using 95% overlap, |Q(t)|would be 20 for most\\nof the song, but it would ramp down to be 1 at the start and\\nend of the song, with |Q(1)|=|Q(L)|= 1. This method is\\nused to obtain the ﬁnal predicted curves for both chorus and\\nboundary activations.\\nTo obtain a binary prediction, we must apply some peak-\\npicking or thresholding heuristics to the predicted activation\\ncurves. However, we observed in our pilot study that the over-\\nall probability values can be very low for some songs that the\\nmodel is less conﬁdent about. Setting a global threshold to bi-\\nnarize the curves could thus lead to no choruses or boundaries\\nbeing detected in these songs.\\nTo avoid this, we develop a more ﬂexible method which\\nmakes use of the relative likelihoods of the segmented curve.\\nThe post-processing includes three phases: (1) select top P\\npeaks from the boundary curve to partition the song into seg-\\nments; (2) calculate the chorus likelihood by averaging the\\nchorus probabilities within each segment; (3) select the top\\nRsegments (by likelihood) as the choruses, and assign the\\nothers as non-choruses. For the ﬁrst phase, we follow the\\npeak-picking method in [9] to select boundary candidates:\\nany boundary having the maximum probability within a 10-\\nsecond non-overlapped window throughout the curve is kept.\\nEach candidate is assigned a boundary score by subtracting\\nthe average of the activation curve in the past 10 and future 5\\nseconds.\\nWe tailorPandRto the dataset, since the annotation\\nguidelines and hence the typical number of segments for each\\ndataset are different. For example, in Harmonix it is pos-\\nsible for two chorus sections to occur back-to-back with a\\nboundary in between, but this arrangement was not possible\\nin the In-House dataset. Accordingly, we calculate θ, the av-\\nerage number of choruses per 3-minutes, from the training setas prior knowledge. We use it to set PandRas follows:\\nP= 2.5×RandR= 2×d×(θ/180) , wheredis the test\\nsong’s duration in seconds. Intuitively, d×(θ/180) is the ex-\\npected number of chorus sections for a test song. Our choice\\nofRthus reﬂects a strategy to slightly over-segment the song\\nat ﬁrst, which is reasonable since adjacent sections with the\\nsame predicted label will be merged.\\n3. EXPERIMENTS\\n3.1. Implementation Details\\nLibROSA [19] is used to extract the log-scaled mel-spectrogram\\nwithD= 96 components. The waveform is resampled at\\n32KHz, and an FFT window of 2048 samples with 1024-\\nsample hop size is applied. For segmenting chunks, we adopt\\na window size of N= 600 frames (19.2 seconds) with a hop\\nsize ofS= 30. In our preliminary experiments, we found the\\nvalue ofSdoes not signiﬁcantly affect the validation accu-\\nracy when it is appropriately small (e.g. <50). Since it is\\nrelated to the amount of data to be processed, increasing S\\ncan reduce the time complexity.\\nWe useα= 0.1, as we observed in the validation that the\\nboundary curve is more difﬁcult to learn. We note that our\\nmodel is not sensitive to αwhenα <0.5. Smallerα, which\\nemphasizes learning the boundary curve, can result in better\\noverall results. This observation makes intuitive sense: there\\nare far fewer positive training examples for boundary frames\\nthan for chorus frames (ratio is smaller than 0.1), so empha-\\nsizing this loss can force the model to be more careful with\\nframes near boundaries, which can eventually help the post-\\nprocessing to make better decisions.\\nOur model is implemented with TensorFlow 1.15 and\\ntrained using the Adam SGD optimizer that minimizes the\\ncross entropy loss. We use a mini-batch of 256 examples\\nand apply batch normalization with momentum 0.9 at every\\nlayer of the network. The initial learning rate is 0.0005 and\\nannealed by half at every 15,000 training steps.\\n3.2. Experimental Settings\\nWe use three datasets to evaluate the proposed approach:\\nthe subset of SALAMI in the “popular” genre (denoted by\\nSALAMI-pop ) [14]; the Harmonix Set [15] with θ=∼3.7\\n(training sets); and an internal music collection (In-House)\\nwithθ=∼2.2 (training sets). SALAMI-pop was used for\\ntesting only, so its θwas never computed or used; the other\\ndatasets were used to conduct 4-fold cross-validation and\\ncross-dataset evaluations.\\nSALAMI-pop contains 210 songs. Since some songs are\\nannotated twice, we treat each annotation of a song as a sepa-\\nrate test case, yielding 320 test cases. For both SALAMI and\\nHarmonix Set, we categorized “pre-chorus” as non-chorus (to\\ndisentangle the build from the true chorus) and “post-chorus”\\nas chorus (since they seem more related to the chorus than to', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='the rest of the song), and merged the segments accordingly.\\nThe In-House dataset was compiled for the purpose of train-\\ning a chorus detector. It contains 2480 full tracks covering\\nmany genres of popular music, including Chinese-pop, J-pop,\\nK-pop, hip-hop, rock, folk, electronic, and instrumental. At\\nleast one chorus section is annotated in each track.\\nWe study the performance of the raw chorus activation\\ncurve using the area under the ROC (AUC), and the ﬁnal bi-\\nnary output using the pairwise F1 score, which is the standard\\nmetric for evaluating music structure analysis [10] and related\\ntasks like beat/downbeat tracking [20].\\nOur main proposed model is named as Temporal model\\n(Section 2), because it predicts the entire temporal activation\\nof a chunk. We also introduce a variant, termed as Scalar\\nmodel, that predicts a scalar chorus and boundary probability\\n(two values) at the center of an input chunk (like in [9, 21]).\\nSpeciﬁcally, we set S= 6, use global pooling in the back-\\nend model, and skip the output merging stage. To study the\\npotential accuracy loss due to the post-processing design, we\\ncreate OracleBound , which uses the ground-truth boundaries\\nand uses the number of choruses for Rto parse the predicted\\nchorus curve of the best-performing Temporal model.\\nWe compare these models to four open-source base-\\nline systems that use existing approaches: pychorus [22],\\nwhich is based on [7], and three algorithms implemented in\\nMSAF [23]. We optimized pychorus using the following\\nheuristics: we modiﬁed it to output up to 4 top candidates\\n(default is one); and, when no chorus is found with an initial\\nreference duration (15 seconds), we iteratively reduce the\\nduration by 3 seconds until it ﬁnds a chorus.\\nMSAF provides implementations of many algorithms for\\nsegmenting songs and grouping segments. None give explicit\\nfunction labels like “verse” or “chorus,” but we can take the\\npredicted segment groups as chorus candidates, and try two\\nheuristics to guess which group represents the choruses: (1)\\nMax-freq : choose the most frequent label as the chorus, and\\n(2)Max-dur : choose the segment group that covers the great-\\nest duration of a song as the choruses. We use the CNMF [24],\\nSCluster [25], and VMO [26] algorithms, all with default set-\\ntings. As Max-dur consistently outperformed Max-freq for\\neach algorithm, we report these results only.\\n3.3. Results and Discussion\\nThe results are summarized in Table 1, where each value is\\nthe mean score averaged over a complete dataset. To per-\\nform cross-dataset (CD) evaluation (e.g., Temporal-HS on IH\\nor SP), we select the best-performing model in terms of F1\\nfrom the four models trained in the cross-validation (CV) (i.e.,\\namong folds of Temporal-HS on HS), and use it to test all the\\nsongs of the other dataset (i.e., IH or SP).\\nWe observe, ﬁrst of all, that our proposed models outper-\\nform the existing ones by a large margin: the worst of the\\nproposed models was, on average, 0.14 greater than the bestMetric AUC F1\\nModel\\\\Test HS IH SP HS IH SP\\nTemporal-HS .827 .767 .723 .692 .624 .602\\nScalar-HS .826 .728 .706 .688 .597 .585\\nTemporal-IH .775 .868 .736 .630 .668 .596\\nScalar-IH .764 .860 .735 .616 .665 .592\\nOracleBound - - - .738 .825 .709\\npychorus .629 .585 .557 .466 .378 .330\\nCNMF [24] .570 .524 .525 .479 .367 .416\\nSCluster [25] .603 .523 .506 .534 .297 .418\\nVMO [26] .455 .463 .481 .272 .229 .277\\nTable 1 . Mean score comparison on the three datasets:\\nHarmonix Set (HS), In-House (IH), and SALAMI-pop (SP).\\nTemporal-‘X’ and Scalar-‘X’ indicate the results of each\\nmodel when trained on dataset ‘X’. Results in bold were ob-\\ntained using 4-fold cross-validation. All results of the pro-\\nposed models (upper 4 rows) are signiﬁcantly greater than re-\\nsults of the existing systems (lower 4) with p-value <10−20.\\nof the baseline models, for both AUC and F1. This outcome\\nvalidates our expectation that “chorusness” could be learned\\nin a supervised fashion. Second, the Temporal models con-\\nsistently outperform their Scalar counterparts; in particular,\\nthe difference between Temporal-HS and Scalar-HS is statis-\\ntically signiﬁcant (p-value <10−5). This indicates that mod-\\neling longer contexts of the activation is a better approach,\\nperhaps because it exploits the temporal dependency of the\\nactivation curves. Third, although training on a dataset tends\\nto improve performance on that dataset, we observe strong\\nCD performance: the CD F1 scores all lie within 0.61 ±0.03\\nacross the three datasets, demonstrating the generalizability\\nof our approach. Since θis ﬁxed by the training set, high CD\\nperformance indicates robustness to different values of θ. On\\nthe other hand, the margin between our results and the Oracle-\\nBound suggests that an orthogonal approach—e.g., one based\\non repetition—could improve the post-processing.\\n4. CONCLUSION AND FUTURE WORK\\nWe have presented a supervised approach to detecting cho-\\nruses in music audio. In experiments, our systems performed\\nbetter than several existing ones, even when trained on other\\ndatasets. With this promising result, we believe that more\\ntypes of segment labels, such as verse, bridge and solo, can\\nbe detected with supervised learning, and with less depen-\\ndence on context. The current model is relatively simple: it\\nonly considers the local context of audio signals. It could be\\nimproved if we use features and techniques to inform it of a\\ngreater context, such as structure features [27], recurrent ar-\\nchitecture and attention modelling [5].', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='5. REFERENCES\\n[1] J. v. Balen, J. A. Burgoyne, F. Wiering, and R. C.\\nVeltkamp, “An analysis of chorus features in popular\\nsong,” in ISMIR , 2013, pp. 107–112.\\n[2] M. Bartsch and G. Wakeﬁeld, “To catch a chorus: using\\nchroma-based representations for audio thumbnailing,”\\ninIEEE Workshop on the Applications of Signal Pro-\\ncessing to Audio and Acoustics , 2001, pp. 15–8.\\n[3] W. Chai and B. Vercoe, “Music thumbnailing via struc-\\ntural analysis,” in ACM Multimedia , 2003, p. 223–226.\\n[4] M. M ¨uller, N. Jiang, and P. Grosche, “A robust ﬁtness\\nmeasure for capturing repetitions in music recordings\\nwith applications to audio thumbnailing,” IEEE Trans-\\nactions on audio, speech, and language processing , vol.\\n21, no. 3, pp. 531–543, 2012.\\n[5] Y . Huang, S. Chou, and Y . Yang, “Pop music high-\\nlighter: Marking the emotion keypoints,” Trans. ISMIR ,\\nvol. 1, no. 1, 2018.\\n[6] M. Goto, “SmartMusicKIOSK: Music listening station\\nwith chorus-search function,” in Proceedings of the\\nACM symposium on User interface software and tech-\\nnology , 2003, pp. 31–40.\\n[7] M. Goto, “A chorus section detection method for musi-\\ncal audio signals and its application to a music listening\\nstation,” IEEE Transactions on Audio, Speech, and Lan-\\nguage Processing , vol. 14, no. 5, pp. 1783–1794, 2006.\\n[8] A. Eronen, “Chorus detection with combined use of\\nMFCC and chroma features and image processing ﬁl-\\nters,” in DAFx , 2007, pp. 229–236.\\n[9] K. Ullrich, J. Schl ¨uter, and T. Grill, “Boundary detection\\nin music structure analysis using convolutional neural\\nnetworks,” in ISMIR , 2014, pp. 417–422.\\n[10] M. M ¨uller, “Music structure analysis,” in Fundamen-\\ntals of Music Processing: Audio, Analysis, Algorithms,\\nApplications , pp. 167–236. Springer International Pub-\\nlishing, 2015.\\n[11] N. Maddage, C. Xu, M. Kankanhalli, and X. Shao,\\n“Content-based music structure analysis with applica-\\ntions to music semantics understanding,” in ACM Mul-\\ntimedia , 2004, pp. 112–119.\\n[12] J. Paulus, “Improving Markov model based music piece\\nstructure labelling with acoustic information,” in ISMIR ,\\n2010, pp. 303–308.\\n[13] G. Shibata, R. Nishikimi, and K. Yoshii, “Music struc-\\nture analysis based on an LSTM-HSMM hybrid model,”\\ninISMIR , 2020, pp. 15–22.[14] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. D. Roure,\\nand J. S. Downie, “Design and creation of a large-scale\\ndatabase of structural annotations,” in ISMIR , 2011,\\nvol. 11, pp. 555–560.\\n[15] O. Nieto, M. McCallum, M. Davies, A. Robertson,\\nA. Stark, and E. Egozy, “The Harmonix Set: Beats,\\ndownbeats, and functional segment annotations of west-\\nern popular music,” in ISMIR , 2019, pp. 565–572.\\n[16] M. J. Bruderer, M. F. Mckinney, and A. Kohlrausch,\\n“The perception of structural boundaries in melody lines\\nof western popular music,” Musicae Scientiae , vol. 13,\\nno. 2, pp. 273–313, 2009.\\n[17] T. Grill and J. Schl ¨uter, “Music Boundary Detec-\\ntion Using Neural Networks on Spectrograms and Self-\\nSimilarity Lag Matrices,” in EUSIPCO , 2015.\\n[18] J. Pons, O. Nieto, M. Prockup, E. M. Schmidt, A. F.\\nEhmann, and X. Serra, “End-to-end learning for music\\naudio tagging at scale,” in ISMIR , 2018, pp. 637–644.\\n[19] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\\nE. Battenberg, and O. Nieto, “librosa: Audio and music\\nsignal analysis in python,” in Proceedings of the 14th\\npython in science conference , 2015, vol. 8.\\n[20] Simon Dixon, “Evaluation of the audio beat tracking\\nsystem beatroot,” Journal of New Music Research , vol.\\n36, no. 1, pp. 39–50, 2007.\\n[21] A. Maezawa, “Music boundary detection based on a\\nhybrid deep model of novelty, homogeneity, repetition\\nand duration,” in ICASSP , 2019, pp. 206–210.\\n[22] V . Jayaram, Finding Choruses in Songs with\\nPython , 2018 (accessed October 20, 2020),\\nhttps://towardsdatascience.com/ﬁnding-choruses-\\nin-songs-with-python-a925165f94a8.\\n[23] O. Nieto and J. P. Bello, “Systematic exploration of\\ncomputational music structure research,” in ISMIR ,\\n2016, pp. 547–553.\\n[24] O. Nieto and T. Jehan, “Convex non-negative matrix\\nfactorization for automatic music structure identiﬁca-\\ntion,” in ICASSP , 2013, pp. 236–240.\\n[25] B. McFee and D. Ellis, “Analyzing song structure with\\nspectral clustering.,” in ISMIR , 2014, pp. 405–410.\\n[26] C. Wang and G. J. Mysore, “Structural segmentation\\nwith the variable markov oracle and boundary adjust-\\nment,” in ICASSP , 2016, pp. 291–295.\\n[27] J. Serra, M. M ¨uller, P. Grosche, and J. L. Arcos, “Un-\\nsupervised detection of music boundaries by time series\\nstructure features,” in AAAI , 2012.', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 4}),\n",
       " Document(page_content='TO CATCH A CHORUS: USING CHROMA-BASED REPRESENTATIONS FOR AUDIO\\nTHUMBNAILING\\nMark A. Bartsch\\nUniversity of Michigan\\nEECS Department\\n1101 Beal Avenue, 143 ATL\\nAnn Arbor, MI 48109-2110\\nmbartsch@eecs.umich.eduGregory H. Wakeﬁeld\\nUniversity of Michigan\\nEECS Department\\n1101 Beal Avenue, 142 ATL\\nAnn Arbor, MI 48109-2110\\nghw@eecs.umich.edu\\nABSTRACT\\nAn important application for use with multimedia databases is a\\nbrowsing aid, which allows a user to quickly and efﬁciently pre-view selections from either a database or from the results of adatabase query. Methods for facilitating browsing, though, are\\nnecessarily media dependent. We present one such method that\\nproduces short, representative samples (or “audio thumbnails”) ofselections of popular music. This method attempts to identify thechorus or refrain of a song by identifying repeated sections of theaudio waveform. A reduced spectral representation of the selection\\nbased on a chroma transformation of the spectrum is used to ﬁnd\\nrepeating patterns. This representation encodes harmonic relation-ships in a signal and thus is ideal for popular music, which is oftencharacterized by prominent harmonic progressions. The method is\\nevaluated over a sizable database of popular music and found to\\nperform well, with most of the errors resulting from songs that donot meet our structural assumptions.\\n1. INTRODUCTION\\nWith the growing prevalence of large databases of multimedia con-\\ntent, the ability to quickly and efﬁciently browse selections from\\nsuch databases is extremely important. This is especially true withadvanced multimedia search and retrieval systems, where the usermust be able to preview hits rapidly to determine their relevance tothe original search. In order to improve the efﬁciency of brows-\\ning, one must consider not only the cost of delivery (in band-\\nwidth, for instance) but also the time required to audition selec-tions. Because of the wide variety of media that one may wishto browse, methods that facilitate such browsing must be media-\\ndependent. The browsing of images, for instance, can be facilitated\\nusing smaller, downsampled versions of the original images. Sim-ilarly, a database that is predominantly comprised of audio record-ings of speech can be well represented with text transcripts or sim-\\nilar summarizations. For music, we propose that a useful reduction\\narises from the identiﬁcation of a short, representative portion ofthe original selection, an “audio thumbnail.”\\nThe identiﬁcation of such a representative sampling is not triv-\\nial in the general case. In classical music, a good “thumbnail”might capture the introduction of a prominent theme or motif. Iden-tifying such an introduction is complicated by the musical context\\nsurrounding it. Much popular music, though, possesses a simpler\\nmusical form that involves the repetition of a chorus or refrain.These repeated sections of the song are typically prominent andare generally sections that are readily recognized or remembered\\nby the listener. Thus, if we can identify the repeated sections in a\\npiece of popular music, we are likely to have also identiﬁed a goodthumbnail. This is a somewhat simpler problem than the generalcase of audio thumbnailing; for this reason, in the present work werestrict our attention to popular music.\\nThe problem of audio thumbnailing has been addressed previ-\\nously by Logan and Chu [1], who developed algorithms for ﬁndingkey phrases in selections of popular music. Their work focused onthe use of Hidden Markov Models and clustering techniques onmel-frequency cepstral coefﬁcients (MFCCs), a set of spectral fea-\\ntures that have been used with great success for applications in\\nspeech processing [2]. Their system was subjectively evaluatedon a relatively small selection of Beatles songs. In another work,Foote [3] identiﬁes this problem, which he calls audio “gisting,” as\\nan application of his measure of audio novelty. This audio novelty\\nscore is based on the similarity matrix, which compares frames ofaudio based on features extracted from the audio. Foote leavesdetails such as the similarity metric and feature class as design de-cisions; however, Foote does recommend the use of MFCCs as a\\nfeature class for computing audio novelty [4].\\nHere we present a new system that automatically generates\\naudio thumbnails for selections of popular music. Our system em-\\nploys a feature-classiﬁcation framework for audio analysis. Thekey aspects of this system are its use of feature similarity for de-\\ntecting musical recurrence and its novel feature class for represent-\\ning musical structure. This feature class represents the spectrum interms of pitch-class, and is derived from the chromagram [5]. In\\nthe following, we present the chromagram and our chroma-based\\nfeature class, outline the operation of our system, and present re-\\nsults to evaluate the performance of our system on a sizable databaseof diverse material.\\n2. THE CHROMAGRAM AND A CHROMA-BASED\\nFEATURE CLASS\\nIn the 1960’s, Shepard reported two distinct attributes of pitch per-\\nception, tone height andchroma [6]. Tone height describes the\\ngeneral increase in the pitch of a sound as its frequency increases.\\nChroma, on the other hand, is cyclic in nature with octave peri-odicity. Under this formulation two tones separated by an integral\\nnumber of octaves share the same value of chroma. This is an in-\\ntuitive concept for musicians, since chroma is closely related tothe musical-theoretic concept of pitch class. Later work by Patter-\\n21-24 October 2001, New Paltz, New York 15', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='−0.8−0.6−0.4−0.20   0.2 0.4 0.6 0.8 1   \\nTime (Seconds)Timeh (Seconds)\\n50 100 150 20050100150200\\nFigure 1: The similarity matrix, C, for Jimmy Buffet’s Margar-\\nitaville , showing the similarity between individual frames of the\\nsong.\\nson [7] suggested that one could decompose frequency into similar\\nattributes.\\nWe have found it useful to employ the musical relevance of\\nchroma in the development of features for our structural patternrecognition. Suppose that we restructure the frequency spectruminto a chroma spectrum [5]. This forms the basis for the chroma-\\ngram. Under such a restructuring, a harmonic series is mapped, in\\na many-to-one fashion, onto a relatively small number of chroma\\nvalues. The ﬁrst twenty harmonics of a harmonic series fall on onlyten different chroma values, while thirteen of those twenty occupyonly four distinct chroma values. This energy compaction is an\\nimportant property of the chromagram. Furthermore, all but four\\nof these ﬁrst twenty harmonics fall within 15 cents of the clos-est “ideal” chroma value of the equal tempered scale. This sug-gests that we might discretize the chromagram into twelve distinct\\nchroma “bins” corresponding to the twelve pitch classes without a\\nsigniﬁcant loss of ﬁdelity in the representation.\\nPerforming this mapping procedure on the spectrum of a frame\\nof audio data provides us with a highly reduced representation of\\nthe frame, consisting of a single twelve-element feature vector.\\nOne of the most useful properties of this feature vector is its abil-ity to encode the harmony within a given song. Thus, two audioframes with similar harmonic content will have similar feature vec-tors. Of course, other aspects of the audio signal are also encoded\\n(instrumentation, for instance, affects the chroma vector as well as\\ncan changes in the “timbre” of an instrument’s sound). With thisfeature vector, we can measure the similarity between two audioframes simply by measuring the correlation between their feature\\nvectors. Then, we can further measure the correlation between ex-\\ntended regions by summing the correlation between their individ-ual frames. This procedure forms the basis for our thumbnailingalgorithm, which is presented in the next section.\\n3. ALGORITHM DESCRIPTION\\n3.1. Frame Segmentation\\nBefore the algorithm begins, we must ﬁrst deﬁne a frame seg-\\nmentation for the song. We have found that using a dynamic,\\n0 5 101520253035\\nLag (Seconds)Time (Seconds)\\n50 100 150 20050100150200\\nFigure 2: The time-lag surface, T, for Jimmy Buffet’s Margari-\\ntaville , showing the similarity between one segment of the song\\nand a segment lagseconds ahead of it.\\nbeat-synchronous frame segmentation improves the system’s per-\\nformance. Thus, as a preprocessing step we apply a beat-tracking\\nalgorithm developed by Simon Dixon [8] to the selection underconsideration. While not perfect, the beat-tracking algorithm per-forms well for a wide variety of popular music. Typically, the\\nresulting frames are on the order of one-quarter to one-half of a\\nsecond in length.\\n3.2. Feature Calculation\\nFor each frame of audio data, we compute a feature vector by cal-\\nculating the logarithmic magnitude of a zero-padded DFT, label-\\ning each DFT bin with the appropriate pitch class, and then taking\\nthe arithmetic mean of all similarly-classiﬁed bins. In the label-ing process, we restrict the frequency range to 20 Hz to 2000 Hz.We have found that this frequency range provides a sufﬁcientlyrich description of the musical sounds that we are considering. We\\nalso subtract the mean of the resulting vector, which normalizes\\nthe feature vector with respect to the original signal’s amplitude.\\n3.3. Correlation Calculation\\nAfter feature calculation, we compute the correlation between each\\npair of vectors calculated in step one. We place the results in a\\nsimilarity matrix that has lines of constant lag oriented along the\\ndiagonals of the matrix. Thus, an extended region of similaritybetween two portions of a song will show up as an extended areaof high correlation along one of the diagonals. One example ofsuch a similarity matrix is shown in Figure 1.\\n3.4. Correlation Filtering\\nIn the next step, we ﬁlter along the diagonals of the similarity ma-\\ntrix to compute similarity between extended regions of the song.The size of these regions is dependent upon the length of the ﬁlter’simpulse response. We use a uniform moving average ﬁlter where\\nthe length of the impulse response is left as a design parameter of\\nthe system. The ﬁltering results are placed in a restructured time-lag matrix, in which the lines of constant lag are oriented along the\\n16  \\nIEEE Workshop on Applications of Signal Processing to Audio and Acoustics 2001', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 1}),\n",
       " Document(page_content='5 10 15 20 25 30 35 40 45 50 55 6000.10.20.30.40.50.60.70.80.91\\nWindow Length (seconds)Score\\nChroma −based\\nMFCC −based\\nRandom\\nFigure 3: Pp(solid) and Pr(dotted) scores for chroma-based algo-\\nrithm, MFCC-based algorithm, and random thumbnail selection.\\ncolumns of the matrix. An example of such a time-lag matrix is\\nshown in Figure 2.\\n3.5. Thumbnail Selection\\nThe ﬁnal step of the algorithm is the selection of the thumbnail\\nitself. This selection occurs by locating the maximum element of\\nthe the time-lag matrix subject to two constraints. To prevent the\\nselection of quick repetitions and fading repeats, we require thatthis location have a lag greater than one-tenth the length of the songand occur less than three-fourths of the way into the song. The\\nthumbnail is then deﬁned by the time-position of this maximum,\\nwhich corresponds to the time that the ﬁrst of the pair of sectionsbegins, and the length of the window used for ﬁltering.\\n4. ALGORITHM EV ALUATION\\nA database of popular music was used to evaluate the performanceof the proposed system. The database is comprised of ninety-three selections of popular music, with styles including rock, folk,\\ndance, country-western, and numerous others. To offset the struc-\\ntural ambiguity of some popular music, we have also included inthe database a number of contemporary Christian hymns with avery clear chorus-verse structure.\\nTo evaluate the output of the system, it is necessary to know\\nwhat portions of a song would make good thumbnails. This is ac-complished by hand-selecting portions of each song as “truth” in-tervals. For the majority of songs, these truth intervals delimit rep-\\netitions of the chorus or refrain of the song. Not all of the songs,\\nhowever, possess a single, clearly deﬁned chorus or refrain. Insuch cases, we select intervals that seem to be representative of thesong. In a few cases, for instance, two equally reasonable candi-dates for a refrain are both selected throughout the song. In others,\\nthe individual verses of the song are identiﬁed. Often these choices\\nare somewhat arbitrary; however, we have attempted to maintainconsistency as much as possible.\\nTwo scoring methods are used to evaluate the algorithm’s out-\\nput. The ﬁrst score, P\\nr, is deﬁned to be the length of the longest\\noverlap between the output interval and a truth interval in the song5 10 15 20 25 30 35 40 45 50 55 6000.10.20.30.40.50.60.70.80.91\\nWindow Length (seconds)Passing Rate0.3\\n0.50.70.80.9\\nFigure 4: The chroma-based algorithm’s passing rate for Pp(solid\\nlines) and Pr(dotted lines) under various thresholds.\\ndivided by the total length of that truth interval. This score is effec-\\ntively a frame-level recall rate. An interval receives a sub-optimalP\\nrscore when the output does not contain all of the relevant cho-\\nrus. The second score, Pp, is deﬁned as the same overlap length\\ndivided by the length of the output interval. This score is effec-tively a measure of frame-level precision. P\\nperrors occur when\\nthe system includes selects portions of a song that are not con-tained in the chorus. Clearly, there is an inherent tradeoff between\\nthese scores. An output interval that is too long will most likely\\nhave a high P\\nrbut a low Pp; if the thumbnail is too short, the\\nsituation is reversed.\\nFigure 3 shows a comparison of the performance of our chroma-\\nbased algorithm versus the same algorithm using mel-frequency\\ncepstral coefﬁcients and also a random selection method. For each\\nmethod, the mean of each score over the entire database is plottedas a function of window size. The MFCCs are calculated at thesame rate as the chroma-based features; aside from the use of adifferent set of features, the algorithm is the same. The random\\nselection method calculates the mean score of 1000 uniformly dis-\\ntributed thumbnails for each song. From this ﬁgure, we can seethat our chroma-based implementation of the algorithm performssubstantially better than the random selection method and the al-\\ngorithm employing MFCCs for both scores and over all of the win-\\ndow lengths plotted here. Also, the expected tradeoff between P\\nr\\nand Ppwith window size is clearly shown. It is interesting to note\\nthat the intersection of all three sets of curves occurs around 23 sec-\\nonds, which is the mean length of truth intervals in our database.\\nThis should not be surprising; if the output selection has the samelength as the overlapping truth interval, P\\nrand Ppwill be identi-\\ncal.\\nAnother useful measure of performance is the fraction of the\\nsongs with a score exceeding some threshold, or the “passing rate.”\\nFigure 4 displays the chroma-based algorithm’s passing rates for\\nboth scores versus window length under various thresholds. Onceagain, the tradeoff between the two scores is evident, and the in-tersection of the two score curves for each threshold lies in the\\nvicinity of 23 seconds. This ﬁgure shows that we can obtain good\\npassing rates for one score without too great an effect on the other\\nif we choose a window size around 20 to 25 seconds.\\nIt is instructive to see when and why the system fails. The\\n21-24 October 2001, New Paltz, New York 17', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='most common failure occurs when the chorus or refrain is repeated,\\nbut there is some change, either in instrumentation or in the mu-\\nsical structure of the repeat. Failure in these cases occurs when\\nthe verses (or some other sections) of the song are more similarto one another than the modiﬁed repetitions of the chorus or re-frain. These cases violate our initial assumption of high correlationbetween instances of the chorus and indicate that this assumption\\nshould be relaxed. A less common error occurs when the repetition\\nof some “uninteresting” portion of the song has a high enough cor-relation enough to overshadow the repetitions of the chorus. Oneexample of this is the repetition of “Hello” in Nirvana’s Smells Like\\nTeen Spirit , which is selected by our algorithm as a good thumb-\\nnail. In other cases this may be an instrumental section, such asthe introduction of the song.\\n5. DISCUSSION\\nWe have shown that our algorithm for selecting audio thumbnailsoperates quite well on one database of popular music. Generally,the system fails when a song does not meet our initial assump-\\ntion that strongly repeated portions of a song correspond to the\\nchorus, refrain, or otherwise important part of a song. We can gen-eralize this cause of error and predict that the system will mostlikely perform poorly on types of music that do not have the sim-ple “verse-refrain” form often found in popular music. We have\\nalready argued that the structure of classical music is too compli-\\ncated to yield readily to this simple approach. Similarly, the im-provisational nature of jazz and blues, for instance, violates ouroriginal assumption as well. Thus we would not expect the system\\nto perform well in those cases.\\nThe most important conclusion that can be drawn from these\\nresults relates to the potential of chroma-based representations for\\nencoding musical structure. While the algorithm does perform bet-ter than chance when using MFCCs as features, the algorithm per-forms signiﬁcantly better when we employ our chroma-based fea-tures. This indicates that these chroma features are a much better\\nchoice for this application. MFCCs are a general purpose mea-\\nsure of the smoothed spectrum of an audio signal which primarilyrepresent the timbral aspects of the sound. Since we are seekingstructure in music, we would prefer to represent the elements of\\nthe music that provide its structure: notes, harmonic progressions,\\nand so on. These elements are encoded in the ﬁne spectral struc-ture of the audio signal and are not well represented by MFCCs.Our chroma-based representation retains and exploits much of this\\nﬁne structure.\\nWe have hypothesized that a chroma-based feature class can\\nbe used to capture harmonic relationships within a song. An ex-amination of the chroma features themselves indicates that they\\ndo encode harmonic relationships to some extent. The features ex-hibit long-term patterns that are indicative the dominant harmoniesin a song and may be useful for determining musical key. Locally,\\none can identify changes that to map to harmonic progressions in\\nthe song. This suggests that a chroma-based feature class could beuseful for any application that requires structural analysis of musicbased on harmonic relationships.\\nThere are a number of interesting extensions that could be\\nmade to this simple method. First, the ability to optimize over win-dow size would be useful to mitigate the tradeoff between the two\\nsources of error. Further, performance would likely be improved\\nif the system could take advantage of more structural informationthan just the highest pairwise correlation. This extension couldalso form the basis of an extension to a full music-segmentationsystem based on structural content, which could be valuable for\\nmusical form analysis. It is also possible that this method would\\nbe a useful technique for multimedia search and retrieval systems.\\n6. CONCLUSION\\nWe have presented a system which uses chroma-based represen-tations of sound to isolate repeated sections within popular musicfor the purpose of producing short, representative samples of en-\\ntire songs. Such a system has numerous applications, including\\nthe browsing of musical databases and multimedia search results.Perhaps more importantly, the success of this system serves to il-lustrate the potential of chroma-based representations for the struc-\\ntural analysis of musical content as an alternative to mel-frequency\\ncepstral coefﬁcients. This system provides a ﬁrst step towardsusing chroma-based representations as an important element ofmore sophisticated analysis systems, including segmentation and\\nsearch-and-retrieval.\\n7. ACKNOWLEDGEMENTS\\nThis material is based upon work supported, in part, by a Na-\\ntional Science Foundation Graduate Research Fellowship and bygrants from the National Science Foundation (IIS-0085945) andthe MusEn Project at the University of Michigan through the Of-\\nﬁce of the Vice President for Research. The authors would like\\nto thank members of the MusEn Project, William Birmingham,Bryan Pardo, Colin Meek, and Maureen Mellody, for their com-ments and contributions to this work. The authors would also like\\nto thank the anonymous reviewers of this paper for their helpful\\nsuggestions for improvements.\\n8. REFERENCES\\n[1] B. Logan and S. Chu, “Music summarization using key\\nphrases,” in International Conference on Acoustics, Speech\\nand Signal Processing , 2000.\\n[2] L. R. Rabiner and B.-H. Juang, Fundamentals of Speech\\nRecognition , Prentice-Hall, 1993.\\n[3] J. Foote, “Automatic audio segmentation using a measure of\\naudio novelty,” in Proceedings of IEEE International Confer-\\nence on Multimedia and Expo , 1999, vol. I, pp. 452–455.\\n[4] J. Foote, “Visualizing music and audio using self-similarity,”\\ninProceedings of ACM Multimedia ’99, Orlando, Florida ,\\nNovember 1999, pp. 77–80.\\n[5] G. H. Wakeﬁeld, “Mathematical representation of joint time-\\nchroma distributions,” in SPIE, Denver, Colorado , 1999.\\n[6] R. Shepard, “Circularity in judgements of relative pitch,”\\nJournal of the Acoustical Society of America , vol. 36, pp.\\n2346–2353, 1964.\\n[7] R. D. Patterson, “Spiral detection of periodicity and the spiral\\nform of musical scales,” in Psychology of Music , chapter 14,\\npp. 44–61. 1986.\\n[8] S. Dixon, “A lightweight multi-agent musical beat tracking\\nsystem,” in Proceedings of the AAAI Workshop on Artiﬁcial\\nIntelligence and Music: Towards Formal Models for Compo-sition, Performance and Analysis . 2000, AAAI Press.\\n18  \\nIEEE Workshop on Applications of Signal Processing to Audio and Acoustics 2001', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/225860996\\nMusic Structu re Analysis from Acoustic Signals\\nChapt er · Januar y 2009\\nDOI: 10.1007/978-0-387-30441-0_21\\nCITATIONS\\n59READS\\n1,113\\n2 author s:\\nRoger B Dannenber g\\nCarne gie Mellon Univ ersity\\n265 PUBLICA TIONS \\xa0\\xa0\\xa05,202  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nMasat aka Got o\\nNational Instit ute of Adv anced Industrial Scienc e and T echnolog y\\n317 PUBLICA TIONS \\xa0\\xa0\\xa07,593  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Roger B Dannenber g on 21 May 2014.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 0}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 1 - Music Structure Analysis from Acoustic Signals Roger B. Dannenberg and Masataka Goto  Abstract Music is full of structure, including sections, sequences of distinct musical textures, and the repetition of phrases or entire sections. The analysis of music audio relies upon feature vectors that convey information about music texture or pitch content. Texture generally refers to the average spectral shape and statistical fluctuation, often reflecting the set of sounding instruments, e.g. strings, vocal, or drums. Pitch content reflects melody and harmony, which is often independent of texture. Structure is found in several ways. Segment boundaries can be detected by observing marked changes in locally averaged texture. Similar sections of music can be detected by clustering segments with similar average textures. The repetition of a sequence of music often marks a logical segment. Repeated phrases and hierarchical structures can be discovered by finding similar sequences of feature vectors within a piece of music. Structure analysis can be used to construct music summaries and to assist music browsing. Introduction Probably everyone would agree that music has structure, but most of the interesting musical information that we perceive lies hidden below the complex surface of the audio signal. From this signal, human listeners perceive vocal and instrumental lines, orchestration, rhythm, harmony, bass lines, and other features. Unfortunately, music audio signals have resisted our attempts to extract this kind of information. Researchers are making progress, but so far, computers have not come near to human levels of performance in detecting notes, processing rhythms, or identifying instruments in a typical (polyphonic) music audio texture. On a longer time scale, listeners can hear structure including the chorus and verse in songs, sections in other types of music, repetition, and other patterns. One might think that without the reliable detection and identification of short-term features such as notes and their sources, that it would be impossible to deduce any information whatsoever about even higher levels of abstraction. Surprisingly, it is possible to automatically detect a great deal of information concerning music structure. For example, it is possible to label the structure of a song as AABA, meaning that opening material (the “A” part) is repeated once, then contrasting material (the “B” part) is played, and then the opening material is played again at the end. This structural description may be deduced from low-level audio signals. Consequently, a computer might locate the “chorus” of a song without having any representation of the melody or rhythm that characterizes the chorus. Underlying almost all work in this area is the concept that structure is induced by the repetition of similar material. This is in contrast to, say, speech recognition, where there is a common understanding of words, their structure, and their meaning. A string of unique words can be understood using prior knowledge of the language. Music, however, has no language or dictionary (although there are certainly known forms and conventions). In general, structure can only arise in music through repetition or systematic transformations of some kind. Repetition implies there is some notion of similarity. Similarity can exist between two points in time (or at least two very short time intervals), similarity can exist between two sequences over longer time intervals, and similarity can exist between the longer-term statistical behaviors of acoustical features. Different approaches to similarity will be described. Similarity can be used to segment music: contiguous regions of similar music can be grouped together into segments. Segments can then be grouped into clusters. The segmentation of a musical work and the grouping of these segments into clusters is a form of analysis or “explanation” of the music. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 1}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 2 - Features and Similarity Measures A variety of approaches are used to measure similarity, but it should be clear that a direct comparison of the waveform data or individual samples will not be useful. Large differences in waveforms can be imperceptible, so we need to derive features of waveform data that are more perceptually meaningful and compare these features with an appropriate measure of similarity.  Feature Vectors for Spectrum, Texture, and Pitch Different features emphasize different aspects of the music. For example, mel-frequency cepstral coefficients (MFCCs) seem to work well when the general shape of the spectrum but not necessarily pitch information is important. MFCCs generally capture overall “texture” or timbral information (what instruments are playing in what general pitch range), but some pitch information is captured, and results depend upon the number of coefficients used as well as the underlying musical signal. When pitch is important, e.g. when searching for similar harmonic sequences, the chromagram is effective. The chromagram is based on the idea that tones separated by octaves have the same perceived value of chroma  (Shepard 1964). Just as we can describe the chroma aspect of pitch, the short term frequency spectrum can be restructured into the chroma spectrum by combining energy at different octaves into just one octave. The chroma vector is a discretized version of the chroma spectrum where energy is summed into 12 log-spaced divisions of the octave corresponding to pitch classes (C, C#, D, … B). By analogy to the spectrogram, the discrete chromagram is a sequence of chroma vectors. It should be noted that there are several variations of the chromagram. The computation typically begins with a short-term Fourier transform (STFT) which is used to compute the magnitude spectrum. There are different ways to “project” this onto the 12-element chroma vector. Each STFT bin can be mapped directly to the most appropriate chroma vector element (Bartsch and Wakefield 2001), or the STFT bin data can be interpolated or windowed to divide the bin value among two neighboring vector elements (Goto 2003a). Log magnitude values can be used to emphasize the presence of low-energy harmonics. Values can also be averaged, summed, or the vector can be computed to conserve the total energy. The chromagram can also be computed by using the Wavelet transform. Regardless of the exact details, the primary attraction of the chroma vector is that, by ignoring octaves, the vector is relatively insensitive to overall spectral energy distribution and thus to timbral variations. However, since fundamental frequencies and lower harmonics of tones feature prominently in the calculation of the chroma vector, it is quite sensitive to pitch class content, making it ideal for the detection of similar harmonic sequences in music. While MFCCs and chroma vectors can be calculated from a single short term Fourier transform, features can also be obtained from longer sequences of spectral frames. Tzanetakis and Cook (1999) use means and variances of a variety of features in a one second window. The features include the spectral centroid, spectral rolloff, spectral flux, and RMS energy.  Peeters, La Burthe, and Rodet (2002) describe “dynamic” features, which model the variation of the short term spectrum over windows of about one second. In this approach, the audio signal is passed through a bank of Mel filters. The time-varying magnitudes of these filter outputs are each analyzed by a short term Fourier transform. The resulting set of features, the Fourier coefficients from each Mel filter output, is large, so a supervised learning scheme is used to find features that maximize the mutual information between feature values and hand-labeled music structures. Measures of Similarity Given a feature vector such as the MFCC or chroma vector, some measure of similarity is needed. One possibility is to compute the (dis)similarity using the Euclidean distance between feature vectors. Euclidean distance will be dependent upon feature magnitude, which is often a measure of the overall ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 3 - music signal energy. To avoid giving more weight to the louder moments of music, feature vectors can be normalized, for example, to a mean of zero and a standard deviation of one or to a maximum element of one. Alternatively, similarity can be measured using the scalar (dot) product of the feature vectors. This measure will be larger when feature vectors have a similar direction. As with Euclidean distance, the scalar product will also vary as a function of the overall magnitude of the feature vectors. If the dot product is normalized by the feature vector magnitudes, the result is equal to the cosine of the angle between the vectors. If the feature vectors are first normalized to have a mean of zero, the cosine angle is equivalent to the correlation, another measure that has been used with success. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) use a constant-Q transform (CQT), and found that CQT outperforms chroma and MFCC features using a cosine distance measure. They also introduce a “structure-based” distance measure that takes into account the harmonic structure of spectra to emphasize pitch similarity over timbral similarity, resulting in additional improvement in a music structure analysis task. Similarity can be calculated between individual feature vectors, as suggested above, but similarity can also be computed over a window of feature vectors. The measure suggested by Foote (1999) is vector correlation:  !\"=++•=10)(1),(wkkjkiwVVwjiS (1) where w is the window size. This measure is appropriate when feature vectors vary with time, forming significant temporal patterns. In some of the work that will be described below, the detection of temporal patterns is viewed as a processing step that takes place after the determination of similarity. Evaluation of Features and Similarity Measures Linear prediction coefficients (LPC) offer another low-dimensional approximation to spectral shape, and other encodings such as moments (centroid, standard deviation, skewness, etc.) are possible. Aucouturier and Sandler (2001) compare various approaches and representations. Their ultimate goal is to segment music according to texture, which they define as the combination of instruments that are playing together. This requires sensitivity to the general spectral shape, and insensitivity to the spectral details that vary according to pitch. They conclude that a vector of about 10 MFCCs is superior to LPC and discrete cepstrum coefficients (Galas and Rodet 1990).  On the other hand, Hu, Dannenberg, and Tzanetakis (2003) compare features for detecting similarity between acoustic and synthesized realizations of a single work of music. In this case, the goal is to ignore timbral differences between acoustic and synthetic instruments, but to achieve fine discrimination of pitches and harmonies. They conclude that the chroma vector is superior to pitch histograms and MFCCs.  Segmentation One approach to discovering structure in music is to locate segments of similar musical material and the boundaries between them. Segmentation does not rely on classification or the discovery of higher order structure in music. However, one can envision using segmentation as a starting point for a number of more complicated tasks, including music summarization, music analysis, music search, and genre classification. Segmentation can also assist in audio browsing, a task that can be enhanced through some sort of visual summary of music and audio segments. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 3}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 4 - Segmentation Using Texture Change Tzanetakis and Cook (1999) perform segmentation as follows: Feature vectors Vi are computed as described above. A feature time differential, Δi, is defined as the Mahalanobis distance:  Δi = (Vi – Vi−1)TΣ-1(Vi – Vi−1) (2) where Σ is an estimate of the feature covariance matrix, calculated from the training data, and i is the frame number (time). This measure is related to the Euclidean distance but takes into account the variance and correlations among features. Next, the first order differences of the distance, Δi − Δi-1, are computed. A large difference indicates a sudden transition. Peaks are picked, beginning with the maximum. After a peak is selected, the peak and its neighborhood are zeroed to avoid picking another peak within the same neighborhood. Assuming the total number of segments is given a priori, the neighborhood is 20% of the average segment size. Additional peaks are selected and zeroed until the desired number of peaks (segment boundaries) has been obtained. Segmentation by Clustering Logan and Chu (2000) describe a clustering technique for discovering music structure. The goal is to label each frame of audio so that frames within similar sections of music will have the same labels. For example, all frames within all occurrences of the chorus should have the same label. This can be accomplished using bottom-up clustering to merge clusters that are similar. Initially, the feature vectors are divided into fixed-length contiguous segments and each segment receives a different label. The following clustering step is iterated: Calculate the mean µ and covariance Σ of the feature vectors within each cluster. Compute a modified Kullback Leibler (KL) distance between each pair of clusters, as described below. Find the pair of clusters with the minimum KL2 distance, and if this distance is below a threshold, combine the clusters. Repeat this step until no distance is below the threshold. The KL2 distance between two Gaussian distributions A and B is given by:  );();(),(2ABKLBAKLBAKL+= (3)  !!\"#$$%&\\'+\\'()+\\'\\'+\\'\\'=BABAABBA11)(µµ (4) Segmentation and Hidden Markov Models Another approach to segmentation uses a hidden Markov model (HMM). In this approach, segments of music correspond to discrete states Q and segment transitions correspond to state changes. Time advances in discrete steps corresponding to feature vectors, and transitions from one state to the next are modeled by a probability distribution that depends only on the current state. This forms a Markov model that generates a sequence of states. Note that states are “hidden” because only feature vectors are observable. Another probability distribution, p(Vi | qi), models the generation of feature vector Vi from state qi. The left side of Figure 1 illustrates a 4-state ergodic Markov model, where arrows represent state transition probabilities. The right side of the figure illustrates the observation generation process, where arrows denote conditional probabilities between variables. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 4}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 5 - dabcstate sequence(hidden)feature vectors(observable)q1q2q3V1V2V3...P(q2|q1)P(V1|q1) Figure 1. Hidden Markov model with four hidden states a, b, c, and d. As shown, feature vectors depend only upon the current state, which depends only upon the previous state. The HMM has advantages for segmentation. In general, feature vectors do not indicate the current state (segment class) unambiguously, so when a single feature vector is observed, one cannot assume that it was generated by particular state. However, some features are more likely to occur in one state than another, so one can observe the trend of feature vectors, ignoring the unlikely outliers and guessing the state that is most consistent with the observations. If transitions are very unlikely, one may have to assume many outliers occur. On the other hand, if transitions are common and segments are short, one can change states rapidly to account for different feature vectors. The HMM formalism can determine the segmentation (the hidden state sequence) with the maximum likelihood given a set of transition probabilities and observations, thus the model can formalize the tradeoffs between minimizing transitions and matching features to states. Furthermore, HMM transition probabilities can be estimated from unlabeled training data, eliminating the need to guess transition probabilities manually. Aucouturier and Sandler (2001) model the observation probability distribution P(Vi|qj) as a mixture of Gaussian distributions over the feature space:  ),,()|(,,1,mjmjMmimjjiVcqVP!\"=#=µN (5) where N is a Gaussian probability density function with mean µi,m, covariance matrix Γj,m, and cj,m is a mixture coefficient. Here, i indexes time and j indexes state. They train the HMM using the Baum-Welch algorithm using the sequence vectors from the single song chosen for analysis. The Viterbi algorithm is then used to find the sequence of hidden states with the maximum likelihood, given the observed feature vectors. \\n012 Figure 2. Segmentation of 20 seconds of a song. State 0 is silence, State 1 is voice, accordion, and accompaniment, and State 2 is accordion and accompaniment. One potential drawback of this approach is that the HMM will segment the signal according to fine-grain changes in spectral content rather than long-term elements of musical form. For example, in one of Aucoturier and Sandler’s test cases (see Figure 2), the HMM segmentation appears to isolate individual words of a singer rather than divide the song according to verses and instrumental interludes. (Aucouturier, Pachet, and Sandler 2005) In other words, the segments can be quite short when there are rapid changes in the music. Although this might be the desired result, it seems likely that one could detect longer-term, higher-level music structure by averaging features over a longer time span or applying further processing to the state sequence obtained from an HMM. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 5}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 6 - Peeters, La Burthe, and Rodet (2002) approach the problem of clustering with a two-pass algorithm. Imagine a human listener hearing a piece of music for the first time. The range of variation of music features becomes apparent, and templates or classes of music are formed. In the second hearing, the structure of the music can be identified in terms of the previously identified templates. An automated system is inspired by this two-pass model. In the first pass, texture change indicates segment boundaries, and “potential” states are formed from the mean values of feature vectors within segments. In the second pass, potential states that are highly similar are merged by using the K-means algorithm. The resulting K states are called the “middle” states. Because they represent clusters with no regard for temporal contiguity, a hidden Markov model initialized with these “middle” states is then used to inhibit rapid inappropriate state transitions by penalizing them. The Baum-Welch algorithm is used to train the model on the sequence of feature vectors from the song. Viterbi decoding is used to obtain a state sequence. Figure 3 shows the result of an analysis using this smoothing technique. HMM State Assignments0123050100150200250TimeState Figure 3. Classification of states in \"Head Over Feet\" from artist Alanis Morisette. (Adapted from Peeters, La Burthe, and Rodet, 2002).  The Similarity Matrix A concept used by many researchers is the similarity matrix. Given a sequence of feature vectors Vi and a measure of similarity S(i, j), one can simply view S(i, j) as a matrix. The matrix can be visualized using a grayscale image where black represents dissimilar vectors and white represents similar vectors. Shades of gray represent intermediate values. Since any vector is similar to itself, the diagonal of the similarity matrix will be white. Also, assuming the similarity measure is symmetric, the matrix will be symmetric about the diagonal. The interesting information in the matrix is in the patterns formed off the diagonal.  In very general terms, there are two interesting sorts of patterns that appear in the similarity matrix, depending on the nature of the features. The first of these appears when features correspond to relatively long-term textures. The second appears when features correspond to detailed short-term features such as pitch or harmony and where similar sequences of features can be observed. These two types of patterns are considered in the next two sections. Texture Patterns First, consider the case where features represent the general texture of the music, for example whether the music is primarily vocal, drum solo, or guitar solo. Figure 4 shows an idealized similarity matrix for this case. The white diagonal appears because feature vectors along the diagonal are identical. Notice that wherever there are similar textures, the matrix is lighter in color (more similar), so for example, all of the feature vectors for the vocals (V) are similar to one another, resulting in large light-colored square patterns both on and off the diagonal. Where two feature vectors correspond to different textures, for example drums and vocals, the matrix is dark. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 6}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 7 - DVGVVDGV\\nGG\\n Figure 4. An idealized similarity matrix for segments of drum (D), vocal (V), and guitar (G) texture. Notice that along the diagonal, a checkerboard pattern appears at segment boundaries, with darker regions to the upper left and lower right, and lighter regions to the lower left and upper right. Foote (2000) proposes the correlation of the similarity matrix S with a kernel based on this checkerboard pattern in order to detect segment boundaries. The general form of the kernel is:  !!!!\"#$$$$%&\\'\\'\\'\\'\\'\\'\\'\\'=1111111111111111C (6) (Note that in Equation 6, row numbers increase in the downward direction whereas in the similarity matrix images, the row number increases in the upward direction. Therefore the diagonal in Equation 6 runs from upper left to lower right.) The kernel image in Figure 5 represents a larger checkerboard pattern with radial smoothing. The correlation N(i) of this kernel along the diagonal of a similarity matrix S can be considered to be a measure of novelty (Foote 2000):  !!\"=\"=++=2/2/2/2/  ),(),()(LLmLLnnimiSnmCiN (7) A graph of N(i) for the similarity matrix in Figure 4 is shown in Figure 5. A peak occurs at each transition because transition boundaries have the highest correlation to the checkerboard pattern.   \\n  Checkerboard Kernel  \\n  Checkerboard Kernel with  Radial Smoothing Correlation Along Diagonal\\n00.20.40.60.81\\n0102030405060Frame OffsetCorrelation Figure 5. The correlation of the kernel shown at lower left with a similarity matrix. Cooper and Foote (2003) extend this technique for finding segment boundaries with a statistical method for clustering segments. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 7}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 8 - Repeating Sequence Patterns While the texture patterns described above are most useful for detecting transitions between segments, the second kind of pattern can be used to discover repetition within a song. For these patterns to appear, it is important that features reflect short-term changes. Generally, features should vary significantly with changes in the pitch of a melody or with changes in harmony. If this condition is satisfied, then there will not be great similarity within a segment, and there will not be a clear pattern of light-colored squares as seen in Figure 4. However, if a segment of music repeats with an offset of j, then S(i, i) will equal S(i, i+j), generating a diagonal line segment at an offset of j from the central diagonal. This is illustrated schematically in Figure 6, where it is assumed that the vocal sections (V) constitute three repetitions of very similar music, whereas the two guitar sections (G) are not so similar. Notice that each non-central diagonal line segment indicates the starting times and the duration of two similar sequences of features. Also, notice that since each pair of similar sequences is represented by two diagonal line segments, there are a total of six (6) off-central line segments in Figure 6. \\nDVGVVDGV\\nGG\\nVV\\n Figure 6. When sections of music are repeated, a pattern of diagonal line segments is generated. Although not shown in Figure 6, the similarity matrix can also illustrate hierarchical relationships. For example, if each vocal section (V) consists of a phrase that is repeated, the similarity matrix would look like the one in Figure 7.  \\nDVGVVDGV\\nGG\\nVV\\n Figure 7. The vocal segments (V) in this similarity matrix contain a repetition, generating additional pattern that is characteristic of music structure. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 8}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 9 - Figure 8 illustrates both texture patterns and repeated sequence patterns from the song “Day Tripper” by the Beatles. The bridge is displayed, starting with three repetitions of a two-measure guitar phrase in the first 11 seconds, followed by six measures of vocals. Notice how a checkerboard pattern appears due to the timbral self-similarity of the guitar section (0 to 11s) and the vocal section (11 to 21s). Finer structure is also visible. A repeated sequence pattern appears within the guitar section as parallel diagonal lines. This figure uses the power spectrum below 5.5kHz as the feature vector, and uses the cosine of the angle between vectors as a measure of similarity. \\n Figure 8. Similarity matrix using spectral features from the bridge of \"Day Tripper\" by the Beatles. The Time-Lag Matrix When the goal is to find repeating sequence patterns, it is sometimes simpler to change coordinate systems so that patterns appear as horizontal or vertical lines. The time-lag matrix r is defined by:   r(t, l) = S(t, t−l), where t−l ≥ 0 (8) Thus, if there is repetition, there will be a sequence of similar frames with a constant lag. Since lag is represented by the vertical axis, a constant lag implies a horizontal line. The time-lag version of Figure 7 is shown in Figure 9. Only the lines representing similar sequences are shown, and the grayscale has been reversed, so that similarity is indicated by black lines. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 9}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 10 - r(t, l)l (lag)\\nt (time)DVGVGV Figure 9. Time-lag matrix representation of the similarity matrix in Figure 7. Finding Repeating Sequences Of course, with audio data obtained from real audio recordings, the similarity or time-lag matrix will be full of noise and ambiguity arising from spurious similarity between different frames. Furthermore, repetitions in music are rarely exact; variation of melodic, harmonic, and rhythmic themes is an essential characteristic of music. In order to automate the discovery of musical structure, algorithms must be developed to identify the structure that lies within the similarity matrix. Melodic Sequence Matching One way to find repetition is to transcribe the melody and perform matching on the resulting symbolic transcription. While extracting the melody from a polyphonic recording (Goto 2004) is very difficult in general, an approximate transcription from an instrumental recording or from a monophonic (melody only) recording is relatively easy. Dannenberg (2002) describes a simple transcription system based on the enhanced autocorrelation algorithm (Tolonen and Karjalainen 2000) applied to a ballad recorded by John Coltrane. The transcription results in a quantized integer pitch value pi and real inter-onset interval di for each note (inter-onset intervals are typically preferred over note duration in music processing). This sequence is processed as follows: 1) First, a similarity matrix is constructed where rows and columns correspond to notes. This differs from the similarity matrix described above where rows and columns correspond to feature vectors with a fixed duration. 2) Each cell of the similarity matrix S(i, j) represents the duration of similar melodic sequences starting at notes i and j. A simple “greedy” algorithm is used to match these two sequences. If note i does not match note j, S(i, j) = 0. 3) Simplify the matrix by removing redundant entries. If a sequence beginning at i matches one at j, then there should be another match at i+1 and j+1. To simplify the matrix, find the submatrix S(i:u, j:v) where the matching sequences at i and j end at u and v. Zero every entry in the submatrix except S(i, j). Also, zero all entries for matching sequences of length 1. 4) Now, any non-zero entry in S represents a pair of matching sequences. By scanning across rows of S we can locate all similar sequences. Sequences are clustered: the first non-zero element in a row represents a cluster of two sequences. Any other non-zero entry in the row that roughly ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 10}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 11 - matches the durations of the clustered sequences is added to the cluster. After scanning the row, all pair-wise matches are zeroed so they will not be considered again. The result of this step is a set of clusters of similar melodic segments. Because repetitions in the music are not exact, there can be considerable overlap between clusters. It is possible for a long segment to be repeated exactly at one offset, and for a portion of that same segment to be repeated several times at other offsets. It may be desirable to simplify the analysis by labeling each note with a particular cluster. This simplification is described in the next section. Simplification or Music Explanation The goal of the simplification step is to produce one possible set of labels for notes. Ideally, the labels should offer a simple “explanation” of the music that highlights repetition within the music. The AABA structure common in songs is a typical explanation. In general, longer sequences of notes are preferable because they explain more, but when sequences are too long, interesting substructure may be lost. For example, the structure AABAAABA could also be represented as AABA repeated, i.e. the structure could be labeled AA, but most theorists would consider this to be a poor explanation. Hierarchical explanations offer a solution, but there is no formal notion as yet of the optimal simplification or explanation. Dannenberg uses a “greedy” algorithm to produce reasonable explanations from first note to last. (Dannenberg and Hu 2002) Notes are initially unlabeled. As each unlabeled note is encountered, search the clusters from the previous section to find one that includes the unlabeled note. If a cluster is found, allocate a new label, e.g. “A”, and label every note included in the cluster accordingly. Continue labeling with the next unlabeled note until all notes are processed. Figure 10 illustrates output from this process. Notice that the program discovered substructure within what would normally be considered the “bridge” or the B part, but this substructure is “real” in the sense that one can see it and hear it. The gap in the middle of the piece is a piano solo where transcription failed. Notice that the program correctly determines that the saxophone enters on the bridge (the B part) after the piano solo. The program also identifies the repeated 2-measure phrase at the end. It fails to notice the structure of ascending pitches at the very end because, while this is a clear musical gesture, it is not based on the repetition of a note sequence. \\n Figure 10. A computer analysis of \"Naima\" by John Coltrane. The automatic transcription appears as a \"piano roll\" at the top, the computer analysis appears as shaded bars, where similar shading indicates similar sequences, and conventional labels appear at the bottom. Finding Similar Sequences in the Similarity Matrix Typically, transcription of a music signal into a sequence of notes is not possible, so similar sequences must be detected as patterns in the similarity or time-lag matrix. For example, Bartsch and Wakefield (2001) filter along diagonals of a similarity matrix to detect similarity. This assumes nearly constant tempo, but that is a good assumption for the popular music used in their study. Their objective was not to identify the beginnings and endings of repeating sequences but to find the chorus of a popular song for use as an “audio thumbnail” or summary. The thumbnail is selected as the maximum element of the filtered similarity matrix, with the additional constraints that the lag is at least one tenth of the length of the song and the thumbnail does not appear in the last quarter of the song. Peeters and Rodet (2003) suggest using a 2D structuring filter on the lag matrix to detect similar sequences. Their filter counts the number of values in the neighborhood to the left and right of a point that ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 12 - are above a threshold. To allow for slight changes in tempo, which results in lines that are not perfectly horizontal, neighbor cells above and below are also considered. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) suggest erosion and dilation operations on the lag matrix to enhance and detect significant similar sequences. Dannenberg and Hu (2003) use a discrete algorithm to find similar sequences and is based on the idea that a path from cell to cell through the similarity matrix specifies an alignment between two subsequences of the feature vectors. If the path goes through S(i, j), then vector i is aligned with j. This suggests using a dynamic time warping (DTW) algorithm (Rabiner and Juang 1993), and the actual algorithm is related to DTW.  The goal is to find alignment paths that maximize the average similarity of the aligned features. A partial or complete path P is defined as a set of pairs of locations and is rated by the average similarity along the path:  !\"=PjijiSPPq),(),(1)( (9) where |P| is the path length using Euclidean distance. Paths are extended as long as the rating remains above a threshold. Paths are constrained to move up one cell, right one cell, or diagonally to the upper right as shown in Figure 11 (and adopting the orientation of the similarity matrix visualizations where time increases vertically and to the right). Therefore, every point that is on a path can be reached from below, from the left, or from the lower left. Each cell (i,j) of an array is computed by looking at the cell below, left, and below left to find the (previously calculated) best path (highest q(P)) passing through those cells. Three new ratings of r are computed by extending each of the three paths to include (i,j). The path with the highest rating is remembered as the one passing through (i,j).  ijj+1i+1 Figure 11. Extending a path from S(i, j). Because cells depend on previously computed values to the lower left, cells are computed along diagonals of constant i+j, from lower left to upper right (increasing i+j). When no path has a rating above some fixed threshold, the path ends. A path may begin wherever S(i, j) is above threshold and no previous paths exist to be extended. Forming Clusters After alignment paths are found, they are grouped into clusters. So if sequence A aligns to sequence B, and sequence A also aligns to sequence C, then A, B, and C should be grouped in a single cluster. Unfortunately, it is unlikely that the alignments of A to B and A to C use exactly the same frames. It is more likely that A aligns to B and A′ aligns to C, where A and A′ are mostly overlapping. This can be handled simply by considering A to equal A′ when they start and end within some fraction of their total length, for example within 10 percent. Once clusters are formed, further simplification and explanation steps can be performed as described above. Isolating Line Segments from the Time-Lag Matrix If nearly constant tempo can be assumed, the alignment path is highly constrained and the alignment path approach may not work well. Taking advantage of the fact that similar sequences are represented by ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 12}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 13 - horizontal lines in the time-lag matrix, Goto (2003a) describes an alternative approach to detecting music structure. In this work, the time-lag matrix is first normalized by subtracting a local mean value while emphasizing horizontal lines. In more detail, given a point r(t,l) in the time-lag matrix, six-directional local mean values along the right, left, upper, lower, upper-right, and lower-left directions starting from r(t,l) are calculated, and the maximum and minimum are obtained. If the local mean along the right or left direction takes the maximum, r(t,l) is considered a part of a horizontal line and emphasized by subtracting the minimum from r(t,l).  Otherwise, r(t,l) is considered a noise and suppressed by subtracting the maximum from r(t,l); noises tend to appear as lines along the upper, lower, upper-right, and lower-left directions. Then, a summary is constructed by integrating over time:  !!dltlrltRtlall\"#=),(),( (10) Rall is then smoothed by a moving average filter along the lag. The result is sketched in Figure 12. Rall is used to decide which lag values should be considered when searching for line segments in the time-lag matrix. A thresholding scheme based on a discriminant criterion is used. The threshold is automatically set to maximize the following between-class variance of the two classes established by the threshold:  221212)(µµ!!\"#=B (11) where ω1 and ω2 are the probabilities of class occurrence (the fraction of peaks in each class), and µ1 and µ2 are the means of peak heights in each class. Each peak above threshold determines a lag value, lp. For each peak, the one-dimensional function r(τ, lp) is searched over lp ≤ τ ≤ t. A smoothing operation is applied to this function and the discriminant criterion of Equation 11 is again used to set the threshold. The result is the beginning and ending points of line segments that indicate repeated sections of music. \\nRall(t, l)r(t, l)l (lag)\\nt (time)DVGVGV Figure 12. The summary Rall(t, l) indicates the possibility that there are similar segments at a lag of l. Modulation Detection A common technique in pop music when repeating a chorus is to change the key, typically modulating upward by half steps. (Note that “modulation” in music is not related to amplitude modulation or frequency modulation in the signal processing sense.) Since modulation changes all the pitches, it is unlikely that a feature vector that is sensitive to pitch sequences could detect any similarity between musical passages in different keys. To a first approximation, a modulation in music corresponds to ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 13}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 14 - frequency scaling, as if changing the speed of a vinyl record turntable or changing the sample rate of a digital recording. On a logarithmic frequency scale, modulation is simply an offset, and when the scale is circular as with pitch classes and chroma, modulation is a rotation. To rotate a vector by ζ, the value of the ith feature is moved to become feature (i + ζ) mod 12. One would expect the chroma vectors for a modulated passage of music to be quite similar to a rotation of the chroma vectors of the unmodulated version. Goto (2003a) exploits this property of the chroma vector by extending the time-lag matrix to incorporate chroma vector rotation by a transposition amount ζ. Denoting !tV as a transposed (rotated) version of a chroma vector tV, rζ(t, l) is the similarity between !tV and the untransposed vector 0ltV!. Since we cannot assume the number of semitones at the modulation in general, the line segment detection is performed on each of 12 versions of rζ(t, l) corresponding to the 12 possible transpositions (this usually does not increase harmful false matches). The segments from all 12 versions are combined to form the set of repeated sections of music, and the transposition information can be saved to form a more complete explanation of the music structure. Chorus Selection after Grouping Line Segments Since each line segment indicates just a pair of repeated contiguous segments, it is necessary to organize into a cluster the line segments that have mostly overlapping frames. When a segment is repeated n times (n ≥ 3), the number of line segments to be grouped in a cluster should theoretically be n(n-1)/2 in the time-lag matrix. Aiming to exhaustively detect all the repeated segments (choruses) appearing in a song, Goto (2003a) describes an algorithm that redetects missing (hidden) line segments to be grouped by top-down processing using information on other detected line segments. The algorithm also appropriately adjusts the start and end times of line segments in each cluster because they are sometimes inconsistent in the bottom-up line-segment detection. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) describe another approach to obtain the best overall combination of segment similarity and duration by adjusting segment boundaries. A cluster corresponding to the chorus can be selected from those clusters. In general, a cluster that has many and long segments tends to be the chorus. In addition to this property, Goto (2003a) uses heuristic rules to select the chorus with a focus on popular music; for example, when a segment has half-length repeated sub-segments, it is likely to be the chorus. The choruslikeness (chorus possibility) of each cluster is computed by taking these rules into account, and the cluster that maximizes the choruslikeness is finally selected. Texture Sequences Detecting repeating patterns in the similarity matrix is equivalent to finding sequences of similar feature vectors. An alternative is to find sequences of similar texture classes. Aucouturier and Sandler (2002) perform a segmentation using hidden Markov models as described earlier. The result is a “texture score,” a sequence of states, e.g. 11222112200, in which patterns can be discovered. They explore two methods for detecting diagonal lines in the similarity matrix. The first is kernel convolution, similar to the filter method of Bartsch and Wakefield (2001). The second uses the Hough Transform (Leavers 1992), a common technique for detecting lines in images. The Hough Transform uses the familiar equation for a line: y = mx+b. A line passing through the point (x,y) must obey the equation b = −mx+y, which forms a line in the (m,b) space. A series of points along the line y = m0x+b0 can be transformed to a series of lines in (m,b) space that all intersect at (m0,b0). Thus, the problem becomes one of finding the intersection of lines. This can be accomplished, for example, by making a sampled two-dimensional image of the (m,b) space and searching for local maxima. It appears that the Hough Transform could be used to find patterns in the similarity matrix as well as in the “texture score” representation. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 15 - One of the interesting features of the texture score representation is that it ignores pitch to a large extent. Thus, music segments that are similar in rhythm and instrumentation can be detected even if the pitches do not match. For example, “Happy Birthday” contains 4 phrases of 6 or 7 notes. There are obvious parallels between these phrases, yet they contain 4 distinct pitch sequences. It seems likely that pitch sequences, texture sequences, rhythmic sequences, and other feature sequences can provide complementary views that will facilitate structure analysis in future systems. Music Summary Browsing images or text is facilitated by the fact that people can shift their gaze from one place to another. The amount of material that is skipped can be controlled by the viewer, and in some cases, the viewer can make a quick scan to search for a particular image or to read headlines. Music, on the other hand, exists in time rather than space. Listeners cannot time-travel to scan a music performance, or experience time more quickly to search for musical “headlines.” At best, one can skip songs or use fast-forward controls with recorded music, but even this is confusing and time-consuming. One application of music structure analysis is to enable the construction of musical “summaries” that give a short overview of the main elements of a musical work. Summaries can help people search for a particular piece of music they know or locate unfamiliar music they might like to hear in full. By analogy to low-resolution versions of images often used to save space or bandwidth, summaries of music are sometimes called “music thumbnails.” Cooper and Foote describe a simple criterion for a music summary of length L: the summary should be maximally similar to the whole. In other words, a summary can be rated by summing the similarity between each feature vector in the summary with each feature vector in the complete work. The rating for the summary beginning at feature vector i is:  !!+===LiimNnLnmSNLiQ1),(1)( (12) The best summary is then the one starting at the value of i the maximizes QL(i). The formula can be extended by weighting S(m,n) to emphasize earlier or louder sections of the song. Other approaches to summary construction are outlined by Peeters, La Burthe, and Rodet (2002). Assume that music has been segmented using one of the techniques described above, resulting in three classes or labels A, B, and C. Some of the interesting approaches to musical summary are: • use the most common class, which in popular music is often the chorus. Some research specifically aims to determine the chorus as described earlier. (Bartsch and Wakefield 2001; Goto 2003a); • use a sample of music from each class, i.e A, B, C. • use examples of each class transition, i.e. A→B, B→A, A→C. In all cases, audio segments are extracted from the original music recording. Unfortunately, artificially and automatically generated transitions can be jarring to listeners. Music structure analysis can help to pick logical points for transitions. In particular, a cut from one phrase of music to a repetition of that phrase can be inaudible. When a cut must be made to a very different texture, it is generally best to make the cut at an existing point of strong textural change. In most music, tempo and meter create a framework that is important for listening. Cuts that jump from the end of one measure to the beginning of another preserve the short-term metrical structure of the original music and help listeners grasp the harmonic and melodic structure more easily. Segments that last 2, 4, or 8 measures (or some duration that relates to the music structure) are more likely to seem “logical” and less disruptive. Thus, music structure analysis is not only important to determine what sections of music to include in a summary, but also to organize those sections in a way that is “musical” and easy for the listener to comprehend. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 16 - An alternative to the construction of “music thumbnails” is to provide a “smart” interface that facilitates manual browsing of entire songs. The SmartMusicKIOSK music listening station (Goto 2003b) displays a time-line with the results of an automatic music structure analysis. In addition to the common stop, pause, play, rewind, and fast forward controls, the SmartMusicKIOSK has controls labeled “next chorus,” “next section,” and “prev section.” (See Figure 13.) These content-based controls allow users to skim rapidly through music and give a graphical overview of the entire music structure, which can be understood without listening to the entire song. \\n Figure 13. The SmartMusicKIOSK user interface showing music structure and structure-related controls. Evaluation Most research in this area has been exploratory, with no means to evaluate whether computer-generated structures and segments are “correct.” In most cases, it is interesting simply to explore what types of structures can be uncovered and what methods can be used. Quantitative evaluations will become more important as problems are better understood and when competing methods need to be compared. Tzanetakis and Cook conducted a pilot study (1999) to compare their automatic segmentation with human segmentation. They found most human subjects agreed on more than half of the segments, and their machine segmentation found more than half of the segments that humans agreed upon. Bartsch and Wakefield (2001) hand-selected “true audio thumbnails” from 93 popular songs and measured “recall,” the fraction of true frames labeled by their program as the chorus, and “precision,” the fraction of labeled chorus frames that  are true frames. With the chorus length set to around 20 to 25 seconds, the average recall and precision is about 70%, compared to about 30% for a chorus interval selected at random. Goto (Goto 2003a) also used hand-labeled choruses in 100 popular songs from the RWC Music Database, a source that enables researchers to work with common test data. (Goto et al. 2002) Goto judged the system output to be correct if the F-measure was more than 0.75. The F-measure is the harmonic mean of recall rate (R) and precision rate (P): F-measure = 2RP/(R+P). The system dealt correctly with 80 out of 100 songs. Evaluating music structure descriptions is difficult. Structure exists at many levels and often exhibits hierarchy. The structure intended by the composer and perhaps determined by a music theorist may not correspond to the perception of the typical listener. Nevertheless, one can ask human subjects to identify pattern and structure in music, look for consistency between subjects, and then compare human descriptions to machine descriptions of music. One can also evaluate the impact of music structure detection upon tasks such a browsing, as in SmartMusicKIOSK (Goto 2003b). ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 16}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 17 - Summary and Conclusions Knowledge of musical structure can be used to construct music summaries, assist with music classification, provide high-level interfaces for music browsing, and offer high-level top-down guidance for further analysis. Automatic analysis of music structure is one source of music meta-data, which is important for digital music libraries.  High-level music structure is generally represented by partitioning the music into segments. Sometimes, segments are labeled to indicate similarity to other segments. There are two main principles used to detect high-level music structure. First, segment boundaries tend to occur when there is a substantial change in musical texture. In other words, this is where the music on either side of the boundary is self-similar, but the two regions differ from each other. Secondly, segments can be located by detecting patterns of repetition within a musical work. It should be noted that the music signal, viewed as a time-domain waveform, is not directly useful for analysis because repetition in music is never exact enough to reproduce phase and amplitude relationships. Therefore, the signal is processed to obtain features that capture useful and more-or-less invariant properties. In the case of texture analysis, features should capture the overall spectral shape and be relatively insensitive to specific pitches. Low-order MFCCs are often used to measure texture similarity. To detect music repetition, features should capture changes in pitch and harmony, ignoring texture which may change from one repetition to the next. The chroma vector is often used in this case. The similarity matrix results from a comparison of all feature vector pairs. The similarity matrix offers an interesting visualization of music, and it has inspired the application of various image-processing techniques to detect music structure. Computing the correlation with a “checkerboard” kernel is one method for detecting texture boundaries. Using filters to detect diagonal lines is one method for detecting repetition. Detecting segment boundaries or music repetition generates individual segments or pairs of segments. Further processing can be used to merge segments into clusters. Hidden Markov models, where each hidden state corresponds to a distinct texture, have been applied to this problem. When music is analyzed using repetitions, the structure can be hierarchical, and the structure is often ambiguous. Standard clustering algorithms assume a set of distinct, fixed items, but with music analysis, the items to be clustered are possibly overlapping segments whose start and end times might be adjustable. Music structure analysis is a rapidly-evolving field of study. Future work will likely explore the integration of existing techniques, combining texture-based with repetition-based segmentation. More sophisticated features including music transcription will offer alternative representations for analysis. Finally, there is the possibility to detect richer structures, including hierarchical patterns of repetition, rhythmic motives, harmonic progressions and key changes, and melodic phrases related by transposition. Acknowledgments The authors wish to thank Jonathan Foote for Figure 8. Jean-Julien Aucotourier, Mark Bartsch, Jonathan Foote, Geoffroy Peeters, Ning Hu, Xavier Rodet, Mark Sandler, George Tzanetakis, and Greg Wakefield have made contributions through their work, discussions, and correspondence. References  Aucouturier, J.-J., F. Pachet, and M. Sandler. 2005. \"\"The Way It Sounds\": Timbre Models for Structural Analysis and Retrieval of Music Signals.\" IEEE Transactions on Multimedia, (to appear).  Aucouturier, J.-J., and M. Sandler. 2001. \"Segmentation of Musical Signals Using Hidden Markov Models.\" In Proceedings of the 110th Convention of the Audio Engineering Society. Audio Engineering Society. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 17}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 18 - Aucouturier, J.-J., and M. Sandler. 2002. \"Finding Repeating Patterns in Acoustic Musical Signals: Applications for Audio Thumbnailing.\" In AES22 International Conference on Virtual, Synthetic and Entertainment Audio. Audio Engineering Society, pp. 412-421. Bartsch, M., and G. H. Wakefield. 2001. \"To Catch a Chorus: Using Chroma-Based Representations For Audio Thumbnailing.\" In Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE, pp. 15-18. Cooper, M., and J. Foote. 2003. \"Summarizing Popular Music via Structural Similarity Analysis.\" In 2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE, pp. 127-130. Dannenberg, R. B. 2002. \"Listening to \"Naima\": An Automated Structural Analysis from Recorded Audio.\" In Proceedings of the 2002 International Computer Music Conference (ICMC 2002). San Francisco: International Computer Music Association, pp. 28-34. Dannenberg, R. B., and N. Hu. 2002. Discovering Musical Structure in Audio Recordings. In Anagnostopoulou, C., et al. eds. Music and Artificial Intelligence, Second International Conference, ICMAI 2002. Berlin: Springer-Verlag, pp. 43-57 Dannenberg, R. B., and N. Hu. 2003. \"Pattern Discovery Techniques for Music Audio.\" Journal of New Music Research, 32(2), 153-164.  Foote, J. 1999. \"Visualizing Music and Audio Using Self-Similarity.\" In Proceedings of ACM Multimedia \\'99. New York: Association for Computing Machinery, pp. 77-80. Foote, J. 2000. \"Automatic Audio Segmentation Using a Measure of Audio Novelty.\" In Proceedings of the International Conference on Multimedia and Expo (ICME 2000). New York: IEEE, pp. 452-455. Galas, T., and X. Rodet. 1990. \"An Improved Cepstral Method for Deconvolution of Source-Filter Systems with Discrete Cepstral: Application to Musical Sounds.\" In 1990 International Computer Music Conference (ICMC 1990). San Francisco: International Computer Music Association, pp. 82-84. Goto, M. 2003a. \"A Chorus-Section Detecting Method for Musical Audio Signals.\" In Proceedings of the 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing Proceedings (ICASSP 2003). New York: IEEE, pp. V-437-440. Goto, M. 2003b. \"SmartMusicKIOSK: Music Listening Station with Chorus-Search Function.\" In Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology (UIST 2003). New York: Association for Computing Machinery, pp. 31-40. Goto, M. 2004. \"A Real-time Music Scene Description System: Predominatn-F0 Estimation for Detecting Melody and Bass Lines in Real-world Audio Signals.\" Speech Communication (ISCA Journal), 43(4), 311-329.  Goto, M., T. Nishimura, H. Hashiguchi, and R. Oka. 2002. \"RWC Music Database: Popular, Classical, and Jazz Music Databases.\" In ISMIR 2002 Conference Proceedings. Paris: IRCAM, pp. 287-288. Hu, N., R. B. Dannenberg, and G. Tzanetakis. 2003. \"Polyphonic Audio Matching and Alignment for Music Retrieval.\" In 2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE, pp. 185-188. Leavers, V. F. 1992. Shape Detection in Computer Vision Using the Hough Transform. Berlin: Springer-Verlag. Logan, B., and S. Chu. 2000. \"Music Summarization Using Key Phrases.\" In Proceedings of the 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing Proceedings (ICASSP 2000). New York: IEEE, pp. II-749-752. Lu, L., M. Wang, and H.-J. Zhang. 2004. \"Repeating Pattern Discovery and Structure Analysis from Acoustic Music Data.\" In Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval. New York: Association for Computing Machinery, pp. 275-282. ', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 18}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 19 - Peeters, G., A. L. Burthe, and X. Rodet. 2002. \"Toward Automatic Audio Summary Generation from Signal Analysis.\" In ISMIR 2002 Conference Proceedings. Paris: IRCAM, pp. 94-100. Peeters, G., and X. Rodet. 2003. \"Signal-based Music Structure Discovery for Music Audio Summary Generation.\" In Proceedings of the 2003 International Computer Music Conference (ICMC 2003). San Francisco: International Computer Music Association, pp. 15-22. Rabiner, L., and B.-H. Juang. 1993. Fundamentals of Speech Recognition. Englewood Cliffs, NJ: Prentice Hall. Shepard, R. 1964. \"Circularity in Judgements of Relative Pitch.\" Journal of the Acoustical Society of America, 36(12), 2346-2353.  Tolonen, T., and M. Karjalainen. 2000. \"A computationally efficient multi-pitch analysis model.\" IEEE Transactions on Speech and Audio Processing, 8(6), 708-716.  Tzanetakis, G., and P. Cook. 1999. \"Multifeature Audio Segmentation for Browsing and Annotation.\" In Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE. \\nView publication stats', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_directory = 'documents/pdf/'\n",
    "\n",
    "def load_pdf_docs(directory):\n",
    "    loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "pdf_documents = load_pdf_docs(pdf_directory)\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29 documents for 3 pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the text \n",
    "on [\"\\n\\n\", \"\\n\", \" \", \"\"] in sequential order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='SUPERVISED CHORUS DETECTION FOR POPULAR MUSIC USING CONVOLUTIONAL\\nNEURAL NETWORK AND MULTI-TASK LEARNING\\nJu-Chiang Wang, Jordan B.L. Smith, Jitong Chen, Xuchen Song, and Yuxuan Wang\\nByteDance\\n{ju-chiang.wang, jordan.smith, chenjitong.1, xuchen.song, wangyuxuan.11 }@bytedance.com\\nABSTRACT\\nThis paper presents a novel supervised approach to de-\\ntecting the chorus segments in popular music. Traditional ap-\\nproaches to this task are mostly unsupervised, with pipelines\\ndesigned to target some quality that is assumed to deﬁne\\n“chorusness,” which usually means seeking the loudest or\\nmost frequently repeated sections. We propose to use a\\nconvolutional neural network with a multi-task learning ob-\\njective, which simultaneously ﬁts two temporal activation\\ncurves: one indicating “chorusness” as a function of time,\\nand the other the location of the boundaries. We also propose\\na post-processing method that jointly takes into account the\\nchorus and boundary predictions to produce binary output.', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='In experiments using three datasets, we compare our system\\nto a set of public implementations of other segmentation and\\nchorus-detection algorithms, and ﬁnd our approach performs\\nsigniﬁcantly better.\\nIndex Terms —Chorus detection, CNN, multi-task learn-\\ning, music structural segmentation.\\n1. INTRODUCTION\\nVerse-chorus song form is a very common structure for popu-\\nlar music. In it, verses alternate with choruses, with the lyrics\\nof the verses varying and the choruses repeating more strictly\\nand more frequently. The authors of [1] cite other generaliza-\\ntions used to deﬁne choruses, including that they are the ‘most\\nprominent’ and ‘most catchy’ sections of a piece. These traits\\nmake it desirable to detect choruses automatically, whether\\nfor generating “thumbnails” [2, 3, 4], for ﬁnding the emo-\\ntional “highlights” of a piece [5], or for enabling convenient\\nnavigation based on the song structure [6].\\nHowever, most previous approaches to chorus detection', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='and thumbnailing [7, 8, 2, 3] are unsupervised. They be-\\ngin with an observation about what typeﬁes chorus sections,\\nand search for them on this basis: e.g., ﬁnding the loudest,\\nmost frequently repeated, and/or the most homogenous sec-\\ntion. Since the deﬁnition of ‘chorus’ is a generalization that\\ndoes not apply in all cases, even a perfectly-designed system\\nof this type will fail to detect the chorus in many songs. A bet-\\nter approach may be to let a model learn what deﬁnes ‘cho-rusness’ from labeled examples; this would allow a system to\\nleverage the timbral and spectral features identiﬁed by [1] in\\na study of what acoustic features differentiate choruses.\\nThis approach, when applied to the related task of music\\nboundary detection by [9], led to a huge leap in the state of the\\nart. Prior segmentation algorithms would generally focus on a\\ndeﬁnable proxy task (e.g., detecting points of change or onsets\\nof repetitions), assisted by sensible heuristics (e.g., rounding', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='boundary estimates to the nearest downbeat). A convolutional\\nneural network (CNN) is trained to detect whether the cen-\\nter of a 16-second input is a boundary. When post-processed\\nwith an appropriate threshold, [9] demonstrated a 10% im-\\nprovement in f-measure over the state of the art.\\nWe propose a similar approach: train a neural network to\\npredict the “chorusness” of an excerpt directly from the au-\\ndio, and without the context of the rest of the song. We train a\\nbinary classiﬁer to predict the “chorusness” of each point in a\\nwindow, and slide this window throughout the song to obtain\\na chorus probability curve. However, this leaves the problem\\nof ﬁnding an appropriate threshold for post-processing. To\\nease this, we propose to jointly model the chorus activation\\nand boundary activation curves, so that the loss on the signals\\naround the boundaries is naturally emphasized. At the infer-\\nence phase, it also eases the process of converting the raw', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='probability curve to a binary output for a song.\\nChorus detection is clearly related to two tasks with a long\\ntradition of MIR research: thumbnailing and music structure\\nanalysis (MSA) [10]. The objective of thumbnailing is to ﬁnd\\na short excerpt of a song that would be an effective preview.\\nHowever, there is no deﬁnition of what makes a good preview;\\n[3] cited several. In practice, thumbnailing systems are evalu-\\nated by testing how often they select all or part of a chorus [2],\\nor whichever segment is repeated most often [4]. Recently,\\n[5] proposed a novel, related objective—to ﬁnd the emotional\\nhighlights of pop songs—and evaluated their system based\\non whether it captured the choruses, which were assumed to\\ncorrespond to the highlights, but their system used a neural\\nnetwork trained to detect emotion, not choruses.\\nIn music structure analysis, it is assumed that one family\\nof segments corresponds to the chorus, but predicting which', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='one is only rarely attempted. We are aware of three prior\\nsystems: [11], who assumed a highly restricted template forarXiv:2103.14253v2  [eess.AS]  21 Apr 2021', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 0}),\n",
       " Document(page_content='Fig. 1 . The system diagram.\\nsong structures and used heuristics to predict labels; [12], who\\npaired a standard structure analysis system with an HMM\\ntrained to label the sections; and [13], published very recently,\\nwho proposed a hierarchical generative model (with section\\nparts generating chord progressions, and these in turn gen-\\nerating observed feature sequences). This last model bene-\\nﬁts from supervision, but still relies on a hand-set strategy of\\ndetecting homogeneity and repetitions, based on handcrafted\\nfeatures (chroma and MFCCs).\\nThe lack of attention paid to chorus detection may be\\ndue to the difﬁculty of obtaining sufﬁcient training data.\\nSALAMI [14] contains 1446 songs, but these come from di-\\nverse genres, so it may be difﬁcult to learn a coherent notion\\nof “chorusness” from it. Introduced in 2019, the Harmonix\\nSet [15] contains 912 songs, 888 with “chorus” sections; it is\\nthe most frequent label, with over 3100 choruses altogether,', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='which is 41% more than the “verse” instances. We also have\\nthe annotated chorus locations for an internal dataset (denoted\\nasIn-House ) of 2480 Asian pop songs. We use these three\\nsources to train or evaluate our system. Since the data sources\\nall have different properties, we investigate the cross-dataset\\nperformance of our system.\\nIn addition to the usefulness of detecting choruses for\\nother applications, the annotations of choruses (that we de-\\npend on) seem more reliable than for other sections. In\\nSALAMI, we observed that if one annotator perceives a seg-\\nment starting at time t, there is a 66% chance that the other\\nannotator placed a boundary at the same time (within 0.5\\nseconds)—but this probability rises to 78% if the boundary\\nmarks the start of a ‘chorus’. This greater agreement could\\nbe the result of choruses having more salient beginnings than\\nother section types [16]. Therefore, the reliability of the\\nannotations makes a supervised system more feasible.', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='2. PROPOSED APPROACH\\nThis section details the three main stages of the system. The\\noverall pipeline is illustrated in Figure 1.2.1. Feature and Label Pre-processing\\nWe use the mel-spectrogram of a song as input. The model\\ntakes a window of Nframes (deﬁned as a chunk ) with a hop\\nsize ofSframes at a time. Note that Nis appropriately large\\nto allow the model to see longer contexts of the audio.\\nThe annotations include the starting and ending time-\\nstamps of each chorus. For each song, we create two types\\nof target labels: a chorus activation curve cand a bound-\\nary activation curve b. For a song of length L, we deﬁne\\nc= [c1,...,c L], withct= 1iftlies within a chorus section,\\nandct= 0 otherwise. To smooth the transitions, half of a\\n2-second wide Hann window is used to ramp from 0 to 1\\nprior to the chorus onset; a similar ramp down is added after\\nthe chorus offset. To create the boundary activation curve,\\nwe convert each boundary instant into a “boundary section”', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='of duration 0.5 seconds, and then apply the same ramp up\\nand down. Thus, each boundary produces a 2.5-second wide\\nbump in b. We use a wider target than in [9] to tolerate\\ngreater deviations from the true boundaries in our case, since\\nour goal is to predict the full extent of the chorus.\\nIn previous works [9, 17], the system models the prob-\\nability of a single target (i.e., a boundary) at the center of\\na chunk. By contrast, we design the system to model the\\nprobabilities of the entire activation curve in the chunk, with\\neach probability aligned with a frame in the mel-spectrogram.\\nThis enables the network to explicitly learn the contextual\\ndependency from the target activation curve. To sum up, a\\nchunk-level training sample for the CNN is represented as\\n{X∈RN×D,c∈RN,b∈RN}, where Xis the mel-\\nspectrogram using Dcomponents.\\n2.2. CNN-based Model\\nThe model is shown in the center part of Figure 1. To fa-\\ncilitate reproducibility, we adopt the model architecture pro-', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='posed in [18], which has shown excellent performance in mu-\\nsic classiﬁcation/tagging tasks. We make three modiﬁcations\\nto meet the requirements of our task: First, we add a tempo-\\nral max-pooling layer prior to the spectrum front-end model\\nto sub-sample the input mel-spectrogram. We use a pool size\\nof [6, 1] with a stride of [6, 1]. To ensure synchronization\\nwith the mel-spectrogram, we also apply median-pooling for\\ncandbwith a pool size of 6 with a stride of 6. Second, we\\nreplace the global pooling (for mean- and max-pooling over\\ntime) with a local pooling at the penultimate layer of the back-\\nend model. A pool size of [24, 1] and a stride of [12, 1] are\\nused. This design serves the need to model the entire temporal\\nactivation curve. Third, we add a ﬁnal dense layer to output\\nthe chorus and boundary predictions, denoted by ˆ c∈RN/6\\nandˆb∈RN/6, respectively. All the model parameters remain\\nthe same as [18] except those mentioned above.', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='To achieve multi-task learning, we calculate the losses for\\nˆ candˆbseparately. Then, the ﬁnal loss is the weighted com-\\nbination:α·loss(ˆ c) + (1−α)·loss(ˆb), whereα∈[0,1]and', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 1}),\n",
       " Document(page_content='loss(·)is a reduce-mean operation that averages the element-\\nwise losses.\\n2.3. Output Merging and Post-processing\\nWe obtain the chunks from a song using a large overlap (e.g.\\n95%), so that during training, the model can see the labels\\nfor multiple times with multiple shifts of mel-spectrogram,\\nwhich is expected to help fast convergence. At the prediction\\nstage, we can merge the predictions of multiple overlapping\\nwindows to improve robustness. We take the average of the\\noverlapped probabilities to obtain the merged activation y[t]\\nat each global time step t∈[1,...,L ]of a song, which can\\nbe formulated as follow:\\ny[t] =1\\n|Q(t)|∑\\ni∈Q(t)ˆyi[m(i,t)], (1)\\nwhere{ˆyi[t′]},t′= [1,...,N ]is the predicted activation of\\nthei-th chunk,m(i,t)is the function that maps a global time\\nsteptto a local time step t′for the thei-th chunk, and the\\nfunctionQ(t)returns the set of chunks that are available at t.\\nFor example, using 95% overlap, |Q(t)|would be 20 for most', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='of the song, but it would ramp down to be 1 at the start and\\nend of the song, with |Q(1)|=|Q(L)|= 1. This method is\\nused to obtain the ﬁnal predicted curves for both chorus and\\nboundary activations.\\nTo obtain a binary prediction, we must apply some peak-\\npicking or thresholding heuristics to the predicted activation\\ncurves. However, we observed in our pilot study that the over-\\nall probability values can be very low for some songs that the\\nmodel is less conﬁdent about. Setting a global threshold to bi-\\nnarize the curves could thus lead to no choruses or boundaries\\nbeing detected in these songs.\\nTo avoid this, we develop a more ﬂexible method which\\nmakes use of the relative likelihoods of the segmented curve.\\nThe post-processing includes three phases: (1) select top P\\npeaks from the boundary curve to partition the song into seg-\\nments; (2) calculate the chorus likelihood by averaging the\\nchorus probabilities within each segment; (3) select the top', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='Rsegments (by likelihood) as the choruses, and assign the\\nothers as non-choruses. For the ﬁrst phase, we follow the\\npeak-picking method in [9] to select boundary candidates:\\nany boundary having the maximum probability within a 10-\\nsecond non-overlapped window throughout the curve is kept.\\nEach candidate is assigned a boundary score by subtracting\\nthe average of the activation curve in the past 10 and future 5\\nseconds.\\nWe tailorPandRto the dataset, since the annotation\\nguidelines and hence the typical number of segments for each\\ndataset are different. For example, in Harmonix it is pos-\\nsible for two chorus sections to occur back-to-back with a\\nboundary in between, but this arrangement was not possible\\nin the In-House dataset. Accordingly, we calculate θ, the av-\\nerage number of choruses per 3-minutes, from the training setas prior knowledge. We use it to set PandRas follows:\\nP= 2.5×RandR= 2×d×(θ/180) , wheredis the test\\nsong’s duration in seconds. Intuitively, d×(θ/180) is the ex-', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='pected number of chorus sections for a test song. Our choice\\nofRthus reﬂects a strategy to slightly over-segment the song\\nat ﬁrst, which is reasonable since adjacent sections with the\\nsame predicted label will be merged.\\n3. EXPERIMENTS\\n3.1. Implementation Details\\nLibROSA [19] is used to extract the log-scaled mel-spectrogram\\nwithD= 96 components. The waveform is resampled at\\n32KHz, and an FFT window of 2048 samples with 1024-\\nsample hop size is applied. For segmenting chunks, we adopt\\na window size of N= 600 frames (19.2 seconds) with a hop\\nsize ofS= 30. In our preliminary experiments, we found the\\nvalue ofSdoes not signiﬁcantly affect the validation accu-\\nracy when it is appropriately small (e.g. <50). Since it is\\nrelated to the amount of data to be processed, increasing S\\ncan reduce the time complexity.\\nWe useα= 0.1, as we observed in the validation that the\\nboundary curve is more difﬁcult to learn. We note that our\\nmodel is not sensitive to αwhenα <0.5. Smallerα, which', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='emphasizes learning the boundary curve, can result in better\\noverall results. This observation makes intuitive sense: there\\nare far fewer positive training examples for boundary frames\\nthan for chorus frames (ratio is smaller than 0.1), so empha-\\nsizing this loss can force the model to be more careful with\\nframes near boundaries, which can eventually help the post-\\nprocessing to make better decisions.\\nOur model is implemented with TensorFlow 1.15 and\\ntrained using the Adam SGD optimizer that minimizes the\\ncross entropy loss. We use a mini-batch of 256 examples\\nand apply batch normalization with momentum 0.9 at every\\nlayer of the network. The initial learning rate is 0.0005 and\\nannealed by half at every 15,000 training steps.\\n3.2. Experimental Settings\\nWe use three datasets to evaluate the proposed approach:\\nthe subset of SALAMI in the “popular” genre (denoted by\\nSALAMI-pop ) [14]; the Harmonix Set [15] with θ=∼3.7\\n(training sets); and an internal music collection (In-House)', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='withθ=∼2.2 (training sets). SALAMI-pop was used for\\ntesting only, so its θwas never computed or used; the other\\ndatasets were used to conduct 4-fold cross-validation and\\ncross-dataset evaluations.\\nSALAMI-pop contains 210 songs. Since some songs are\\nannotated twice, we treat each annotation of a song as a sepa-\\nrate test case, yielding 320 test cases. For both SALAMI and\\nHarmonix Set, we categorized “pre-chorus” as non-chorus (to\\ndisentangle the build from the true chorus) and “post-chorus”\\nas chorus (since they seem more related to the chorus than to', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 2}),\n",
       " Document(page_content='the rest of the song), and merged the segments accordingly.\\nThe In-House dataset was compiled for the purpose of train-\\ning a chorus detector. It contains 2480 full tracks covering\\nmany genres of popular music, including Chinese-pop, J-pop,\\nK-pop, hip-hop, rock, folk, electronic, and instrumental. At\\nleast one chorus section is annotated in each track.\\nWe study the performance of the raw chorus activation\\ncurve using the area under the ROC (AUC), and the ﬁnal bi-\\nnary output using the pairwise F1 score, which is the standard\\nmetric for evaluating music structure analysis [10] and related\\ntasks like beat/downbeat tracking [20].\\nOur main proposed model is named as Temporal model\\n(Section 2), because it predicts the entire temporal activation\\nof a chunk. We also introduce a variant, termed as Scalar\\nmodel, that predicts a scalar chorus and boundary probability\\n(two values) at the center of an input chunk (like in [9, 21]).\\nSpeciﬁcally, we set S= 6, use global pooling in the back-', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='end model, and skip the output merging stage. To study the\\npotential accuracy loss due to the post-processing design, we\\ncreate OracleBound , which uses the ground-truth boundaries\\nand uses the number of choruses for Rto parse the predicted\\nchorus curve of the best-performing Temporal model.\\nWe compare these models to four open-source base-\\nline systems that use existing approaches: pychorus [22],\\nwhich is based on [7], and three algorithms implemented in\\nMSAF [23]. We optimized pychorus using the following\\nheuristics: we modiﬁed it to output up to 4 top candidates\\n(default is one); and, when no chorus is found with an initial\\nreference duration (15 seconds), we iteratively reduce the\\nduration by 3 seconds until it ﬁnds a chorus.\\nMSAF provides implementations of many algorithms for\\nsegmenting songs and grouping segments. None give explicit\\nfunction labels like “verse” or “chorus,” but we can take the\\npredicted segment groups as chorus candidates, and try two', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='heuristics to guess which group represents the choruses: (1)\\nMax-freq : choose the most frequent label as the chorus, and\\n(2)Max-dur : choose the segment group that covers the great-\\nest duration of a song as the choruses. We use the CNMF [24],\\nSCluster [25], and VMO [26] algorithms, all with default set-\\ntings. As Max-dur consistently outperformed Max-freq for\\neach algorithm, we report these results only.\\n3.3. Results and Discussion\\nThe results are summarized in Table 1, where each value is\\nthe mean score averaged over a complete dataset. To per-\\nform cross-dataset (CD) evaluation (e.g., Temporal-HS on IH\\nor SP), we select the best-performing model in terms of F1\\nfrom the four models trained in the cross-validation (CV) (i.e.,\\namong folds of Temporal-HS on HS), and use it to test all the\\nsongs of the other dataset (i.e., IH or SP).\\nWe observe, ﬁrst of all, that our proposed models outper-\\nform the existing ones by a large margin: the worst of the', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='proposed models was, on average, 0.14 greater than the bestMetric AUC F1\\nModel\\\\Test HS IH SP HS IH SP\\nTemporal-HS .827 .767 .723 .692 .624 .602\\nScalar-HS .826 .728 .706 .688 .597 .585\\nTemporal-IH .775 .868 .736 .630 .668 .596\\nScalar-IH .764 .860 .735 .616 .665 .592\\nOracleBound - - - .738 .825 .709\\npychorus .629 .585 .557 .466 .378 .330\\nCNMF [24] .570 .524 .525 .479 .367 .416\\nSCluster [25] .603 .523 .506 .534 .297 .418\\nVMO [26] .455 .463 .481 .272 .229 .277\\nTable 1 . Mean score comparison on the three datasets:\\nHarmonix Set (HS), In-House (IH), and SALAMI-pop (SP).\\nTemporal-‘X’ and Scalar-‘X’ indicate the results of each\\nmodel when trained on dataset ‘X’. Results in bold were ob-\\ntained using 4-fold cross-validation. All results of the pro-\\nposed models (upper 4 rows) are signiﬁcantly greater than re-\\nsults of the existing systems (lower 4) with p-value <10−20.\\nof the baseline models, for both AUC and F1. This outcome\\nvalidates our expectation that “chorusness” could be learned', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='in a supervised fashion. Second, the Temporal models con-\\nsistently outperform their Scalar counterparts; in particular,\\nthe difference between Temporal-HS and Scalar-HS is statis-\\ntically signiﬁcant (p-value <10−5). This indicates that mod-\\neling longer contexts of the activation is a better approach,\\nperhaps because it exploits the temporal dependency of the\\nactivation curves. Third, although training on a dataset tends\\nto improve performance on that dataset, we observe strong\\nCD performance: the CD F1 scores all lie within 0.61 ±0.03\\nacross the three datasets, demonstrating the generalizability\\nof our approach. Since θis ﬁxed by the training set, high CD\\nperformance indicates robustness to different values of θ. On\\nthe other hand, the margin between our results and the Oracle-\\nBound suggests that an orthogonal approach—e.g., one based\\non repetition—could improve the post-processing.\\n4. CONCLUSION AND FUTURE WORK\\nWe have presented a supervised approach to detecting cho-', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='ruses in music audio. In experiments, our systems performed\\nbetter than several existing ones, even when trained on other\\ndatasets. With this promising result, we believe that more\\ntypes of segment labels, such as verse, bridge and solo, can\\nbe detected with supervised learning, and with less depen-\\ndence on context. The current model is relatively simple: it\\nonly considers the local context of audio signals. It could be\\nimproved if we use features and techniques to inform it of a\\ngreater context, such as structure features [27], recurrent ar-\\nchitecture and attention modelling [5].', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 3}),\n",
       " Document(page_content='5. REFERENCES\\n[1] J. v. Balen, J. A. Burgoyne, F. Wiering, and R. C.\\nVeltkamp, “An analysis of chorus features in popular\\nsong,” in ISMIR , 2013, pp. 107–112.\\n[2] M. Bartsch and G. Wakeﬁeld, “To catch a chorus: using\\nchroma-based representations for audio thumbnailing,”\\ninIEEE Workshop on the Applications of Signal Pro-\\ncessing to Audio and Acoustics , 2001, pp. 15–8.\\n[3] W. Chai and B. Vercoe, “Music thumbnailing via struc-\\ntural analysis,” in ACM Multimedia , 2003, p. 223–226.\\n[4] M. M ¨uller, N. Jiang, and P. Grosche, “A robust ﬁtness\\nmeasure for capturing repetitions in music recordings\\nwith applications to audio thumbnailing,” IEEE Trans-\\nactions on audio, speech, and language processing , vol.\\n21, no. 3, pp. 531–543, 2012.\\n[5] Y . Huang, S. Chou, and Y . Yang, “Pop music high-\\nlighter: Marking the emotion keypoints,” Trans. ISMIR ,\\nvol. 1, no. 1, 2018.\\n[6] M. Goto, “SmartMusicKIOSK: Music listening station\\nwith chorus-search function,” in Proceedings of the', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 4}),\n",
       " Document(page_content='ACM symposium on User interface software and tech-\\nnology , 2003, pp. 31–40.\\n[7] M. Goto, “A chorus section detection method for musi-\\ncal audio signals and its application to a music listening\\nstation,” IEEE Transactions on Audio, Speech, and Lan-\\nguage Processing , vol. 14, no. 5, pp. 1783–1794, 2006.\\n[8] A. Eronen, “Chorus detection with combined use of\\nMFCC and chroma features and image processing ﬁl-\\nters,” in DAFx , 2007, pp. 229–236.\\n[9] K. Ullrich, J. Schl ¨uter, and T. Grill, “Boundary detection\\nin music structure analysis using convolutional neural\\nnetworks,” in ISMIR , 2014, pp. 417–422.\\n[10] M. M ¨uller, “Music structure analysis,” in Fundamen-\\ntals of Music Processing: Audio, Analysis, Algorithms,\\nApplications , pp. 167–236. Springer International Pub-\\nlishing, 2015.\\n[11] N. Maddage, C. Xu, M. Kankanhalli, and X. Shao,\\n“Content-based music structure analysis with applica-\\ntions to music semantics understanding,” in ACM Mul-\\ntimedia , 2004, pp. 112–119.', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 4}),\n",
       " Document(page_content='[12] J. Paulus, “Improving Markov model based music piece\\nstructure labelling with acoustic information,” in ISMIR ,\\n2010, pp. 303–308.\\n[13] G. Shibata, R. Nishikimi, and K. Yoshii, “Music struc-\\nture analysis based on an LSTM-HSMM hybrid model,”\\ninISMIR , 2020, pp. 15–22.[14] J. B. L. Smith, J. A. Burgoyne, I. Fujinaga, D. D. Roure,\\nand J. S. Downie, “Design and creation of a large-scale\\ndatabase of structural annotations,” in ISMIR , 2011,\\nvol. 11, pp. 555–560.\\n[15] O. Nieto, M. McCallum, M. Davies, A. Robertson,\\nA. Stark, and E. Egozy, “The Harmonix Set: Beats,\\ndownbeats, and functional segment annotations of west-\\nern popular music,” in ISMIR , 2019, pp. 565–572.\\n[16] M. J. Bruderer, M. F. Mckinney, and A. Kohlrausch,\\n“The perception of structural boundaries in melody lines\\nof western popular music,” Musicae Scientiae , vol. 13,\\nno. 2, pp. 273–313, 2009.\\n[17] T. Grill and J. Schl ¨uter, “Music Boundary Detec-\\ntion Using Neural Networks on Spectrograms and Self-', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 4}),\n",
       " Document(page_content='Similarity Lag Matrices,” in EUSIPCO , 2015.\\n[18] J. Pons, O. Nieto, M. Prockup, E. M. Schmidt, A. F.\\nEhmann, and X. Serra, “End-to-end learning for music\\naudio tagging at scale,” in ISMIR , 2018, pp. 637–644.\\n[19] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar,\\nE. Battenberg, and O. Nieto, “librosa: Audio and music\\nsignal analysis in python,” in Proceedings of the 14th\\npython in science conference , 2015, vol. 8.\\n[20] Simon Dixon, “Evaluation of the audio beat tracking\\nsystem beatroot,” Journal of New Music Research , vol.\\n36, no. 1, pp. 39–50, 2007.\\n[21] A. Maezawa, “Music boundary detection based on a\\nhybrid deep model of novelty, homogeneity, repetition\\nand duration,” in ICASSP , 2019, pp. 206–210.\\n[22] V . Jayaram, Finding Choruses in Songs with\\nPython , 2018 (accessed October 20, 2020),\\nhttps://towardsdatascience.com/ﬁnding-choruses-\\nin-songs-with-python-a925165f94a8.\\n[23] O. Nieto and J. P. Bello, “Systematic exploration of', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 4}),\n",
       " Document(page_content='computational music structure research,” in ISMIR ,\\n2016, pp. 547–553.\\n[24] O. Nieto and T. Jehan, “Convex non-negative matrix\\nfactorization for automatic music structure identiﬁca-\\ntion,” in ICASSP , 2013, pp. 236–240.\\n[25] B. McFee and D. Ellis, “Analyzing song structure with\\nspectral clustering.,” in ISMIR , 2014, pp. 405–410.\\n[26] C. Wang and G. J. Mysore, “Structural segmentation\\nwith the variable markov oracle and boundary adjust-\\nment,” in ICASSP , 2016, pp. 291–295.\\n[27] J. Serra, M. M ¨uller, P. Grosche, and J. L. Arcos, “Un-\\nsupervised detection of music boundaries by time series\\nstructure features,” in AAAI , 2012.', metadata={'source': 'documents/pdf/210314253.pdf', 'page': 4}),\n",
       " Document(page_content='TO CATCH A CHORUS: USING CHROMA-BASED REPRESENTATIONS FOR AUDIO\\nTHUMBNAILING\\nMark A. Bartsch\\nUniversity of Michigan\\nEECS Department\\n1101 Beal Avenue, 143 ATL\\nAnn Arbor, MI 48109-2110\\nmbartsch@eecs.umich.eduGregory H. Wakeﬁeld\\nUniversity of Michigan\\nEECS Department\\n1101 Beal Avenue, 142 ATL\\nAnn Arbor, MI 48109-2110\\nghw@eecs.umich.edu\\nABSTRACT\\nAn important application for use with multimedia databases is a\\nbrowsing aid, which allows a user to quickly and efﬁciently pre-view selections from either a database or from the results of adatabase query. Methods for facilitating browsing, though, are\\nnecessarily media dependent. We present one such method that\\nproduces short, representative samples (or “audio thumbnails”) ofselections of popular music. This method attempts to identify thechorus or refrain of a song by identifying repeated sections of theaudio waveform. A reduced spectral representation of the selection\\nbased on a chroma transformation of the spectrum is used to ﬁnd', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='repeating patterns. This representation encodes harmonic relation-ships in a signal and thus is ideal for popular music, which is oftencharacterized by prominent harmonic progressions. The method is\\nevaluated over a sizable database of popular music and found to\\nperform well, with most of the errors resulting from songs that donot meet our structural assumptions.\\n1. INTRODUCTION\\nWith the growing prevalence of large databases of multimedia con-\\ntent, the ability to quickly and efﬁciently browse selections from\\nsuch databases is extremely important. This is especially true withadvanced multimedia search and retrieval systems, where the usermust be able to preview hits rapidly to determine their relevance tothe original search. In order to improve the efﬁciency of brows-\\ning, one must consider not only the cost of delivery (in band-', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='width, for instance) but also the time required to audition selec-tions. Because of the wide variety of media that one may wishto browse, methods that facilitate such browsing must be media-\\ndependent. The browsing of images, for instance, can be facilitated\\nusing smaller, downsampled versions of the original images. Sim-ilarly, a database that is predominantly comprised of audio record-ings of speech can be well represented with text transcripts or sim-\\nilar summarizations. For music, we propose that a useful reduction\\narises from the identiﬁcation of a short, representative portion ofthe original selection, an “audio thumbnail.”\\nThe identiﬁcation of such a representative sampling is not triv-\\nial in the general case. In classical music, a good “thumbnail”might capture the introduction of a prominent theme or motif. Iden-tifying such an introduction is complicated by the musical context\\nsurrounding it. Much popular music, though, possesses a simpler', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='musical form that involves the repetition of a chorus or refrain.These repeated sections of the song are typically prominent andare generally sections that are readily recognized or remembered\\nby the listener. Thus, if we can identify the repeated sections in a\\npiece of popular music, we are likely to have also identiﬁed a goodthumbnail. This is a somewhat simpler problem than the generalcase of audio thumbnailing; for this reason, in the present work werestrict our attention to popular music.\\nThe problem of audio thumbnailing has been addressed previ-\\nously by Logan and Chu [1], who developed algorithms for ﬁndingkey phrases in selections of popular music. Their work focused onthe use of Hidden Markov Models and clustering techniques onmel-frequency cepstral coefﬁcients (MFCCs), a set of spectral fea-\\ntures that have been used with great success for applications in', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='speech processing [2]. Their system was subjectively evaluatedon a relatively small selection of Beatles songs. In another work,Foote [3] identiﬁes this problem, which he calls audio “gisting,” as\\nan application of his measure of audio novelty. This audio novelty\\nscore is based on the similarity matrix, which compares frames ofaudio based on features extracted from the audio. Foote leavesdetails such as the similarity metric and feature class as design de-cisions; however, Foote does recommend the use of MFCCs as a\\nfeature class for computing audio novelty [4].\\nHere we present a new system that automatically generates\\naudio thumbnails for selections of popular music. Our system em-\\nploys a feature-classiﬁcation framework for audio analysis. Thekey aspects of this system are its use of feature similarity for de-\\ntecting musical recurrence and its novel feature class for represent-', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='ing musical structure. This feature class represents the spectrum interms of pitch-class, and is derived from the chromagram [5]. In\\nthe following, we present the chromagram and our chroma-based\\nfeature class, outline the operation of our system, and present re-\\nsults to evaluate the performance of our system on a sizable databaseof diverse material.\\n2. THE CHROMAGRAM AND A CHROMA-BASED\\nFEATURE CLASS\\nIn the 1960’s, Shepard reported two distinct attributes of pitch per-\\nception, tone height andchroma [6]. Tone height describes the\\ngeneral increase in the pitch of a sound as its frequency increases.\\nChroma, on the other hand, is cyclic in nature with octave peri-odicity. Under this formulation two tones separated by an integral\\nnumber of octaves share the same value of chroma. This is an in-\\ntuitive concept for musicians, since chroma is closely related tothe musical-theoretic concept of pitch class. Later work by Patter-\\n21-24 October 2001, New Paltz, New York 15', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 0}),\n",
       " Document(page_content='−0.8−0.6−0.4−0.20   0.2 0.4 0.6 0.8 1   \\nTime (Seconds)Timeh (Seconds)\\n50 100 150 20050100150200\\nFigure 1: The similarity matrix, C, for Jimmy Buffet’s Margar-\\nitaville , showing the similarity between individual frames of the\\nsong.\\nson [7] suggested that one could decompose frequency into similar\\nattributes.\\nWe have found it useful to employ the musical relevance of\\nchroma in the development of features for our structural patternrecognition. Suppose that we restructure the frequency spectruminto a chroma spectrum [5]. This forms the basis for the chroma-\\ngram. Under such a restructuring, a harmonic series is mapped, in\\na many-to-one fashion, onto a relatively small number of chroma\\nvalues. The ﬁrst twenty harmonics of a harmonic series fall on onlyten different chroma values, while thirteen of those twenty occupyonly four distinct chroma values. This energy compaction is an\\nimportant property of the chromagram. Furthermore, all but four', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 1}),\n",
       " Document(page_content='of these ﬁrst twenty harmonics fall within 15 cents of the clos-est “ideal” chroma value of the equal tempered scale. This sug-gests that we might discretize the chromagram into twelve distinct\\nchroma “bins” corresponding to the twelve pitch classes without a\\nsigniﬁcant loss of ﬁdelity in the representation.\\nPerforming this mapping procedure on the spectrum of a frame\\nof audio data provides us with a highly reduced representation of\\nthe frame, consisting of a single twelve-element feature vector.\\nOne of the most useful properties of this feature vector is its abil-ity to encode the harmony within a given song. Thus, two audioframes with similar harmonic content will have similar feature vec-tors. Of course, other aspects of the audio signal are also encoded\\n(instrumentation, for instance, affects the chroma vector as well as', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 1}),\n",
       " Document(page_content='can changes in the “timbre” of an instrument’s sound). With thisfeature vector, we can measure the similarity between two audioframes simply by measuring the correlation between their feature\\nvectors. Then, we can further measure the correlation between ex-\\ntended regions by summing the correlation between their individ-ual frames. This procedure forms the basis for our thumbnailingalgorithm, which is presented in the next section.\\n3. ALGORITHM DESCRIPTION\\n3.1. Frame Segmentation\\nBefore the algorithm begins, we must ﬁrst deﬁne a frame seg-\\nmentation for the song. We have found that using a dynamic,\\n0 5 101520253035\\nLag (Seconds)Time (Seconds)\\n50 100 150 20050100150200\\nFigure 2: The time-lag surface, T, for Jimmy Buffet’s Margari-\\ntaville , showing the similarity between one segment of the song\\nand a segment lagseconds ahead of it.\\nbeat-synchronous frame segmentation improves the system’s per-\\nformance. Thus, as a preprocessing step we apply a beat-tracking', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 1}),\n",
       " Document(page_content='algorithm developed by Simon Dixon [8] to the selection underconsideration. While not perfect, the beat-tracking algorithm per-forms well for a wide variety of popular music. Typically, the\\nresulting frames are on the order of one-quarter to one-half of a\\nsecond in length.\\n3.2. Feature Calculation\\nFor each frame of audio data, we compute a feature vector by cal-\\nculating the logarithmic magnitude of a zero-padded DFT, label-\\ning each DFT bin with the appropriate pitch class, and then taking\\nthe arithmetic mean of all similarly-classiﬁed bins. In the label-ing process, we restrict the frequency range to 20 Hz to 2000 Hz.We have found that this frequency range provides a sufﬁcientlyrich description of the musical sounds that we are considering. We\\nalso subtract the mean of the resulting vector, which normalizes\\nthe feature vector with respect to the original signal’s amplitude.\\n3.3. Correlation Calculation\\nAfter feature calculation, we compute the correlation between each', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 1}),\n",
       " Document(page_content='pair of vectors calculated in step one. We place the results in a\\nsimilarity matrix that has lines of constant lag oriented along the\\ndiagonals of the matrix. Thus, an extended region of similaritybetween two portions of a song will show up as an extended areaof high correlation along one of the diagonals. One example ofsuch a similarity matrix is shown in Figure 1.\\n3.4. Correlation Filtering\\nIn the next step, we ﬁlter along the diagonals of the similarity ma-\\ntrix to compute similarity between extended regions of the song.The size of these regions is dependent upon the length of the ﬁlter’simpulse response. We use a uniform moving average ﬁlter where\\nthe length of the impulse response is left as a design parameter of\\nthe system. The ﬁltering results are placed in a restructured time-lag matrix, in which the lines of constant lag are oriented along the\\n16  \\nIEEE Workshop on Applications of Signal Processing to Audio and Acoustics 2001', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 1}),\n",
       " Document(page_content='5 10 15 20 25 30 35 40 45 50 55 6000.10.20.30.40.50.60.70.80.91\\nWindow Length (seconds)Score\\nChroma −based\\nMFCC −based\\nRandom\\nFigure 3: Pp(solid) and Pr(dotted) scores for chroma-based algo-\\nrithm, MFCC-based algorithm, and random thumbnail selection.\\ncolumns of the matrix. An example of such a time-lag matrix is\\nshown in Figure 2.\\n3.5. Thumbnail Selection\\nThe ﬁnal step of the algorithm is the selection of the thumbnail\\nitself. This selection occurs by locating the maximum element of\\nthe the time-lag matrix subject to two constraints. To prevent the\\nselection of quick repetitions and fading repeats, we require thatthis location have a lag greater than one-tenth the length of the songand occur less than three-fourths of the way into the song. The\\nthumbnail is then deﬁned by the time-position of this maximum,\\nwhich corresponds to the time that the ﬁrst of the pair of sectionsbegins, and the length of the window used for ﬁltering.\\n4. ALGORITHM EV ALUATION', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='A database of popular music was used to evaluate the performanceof the proposed system. The database is comprised of ninety-three selections of popular music, with styles including rock, folk,\\ndance, country-western, and numerous others. To offset the struc-\\ntural ambiguity of some popular music, we have also included inthe database a number of contemporary Christian hymns with avery clear chorus-verse structure.\\nTo evaluate the output of the system, it is necessary to know\\nwhat portions of a song would make good thumbnails. This is ac-complished by hand-selecting portions of each song as “truth” in-tervals. For the majority of songs, these truth intervals delimit rep-\\netitions of the chorus or refrain of the song. Not all of the songs,', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='however, possess a single, clearly deﬁned chorus or refrain. Insuch cases, we select intervals that seem to be representative of thesong. In a few cases, for instance, two equally reasonable candi-dates for a refrain are both selected throughout the song. In others,\\nthe individual verses of the song are identiﬁed. Often these choices\\nare somewhat arbitrary; however, we have attempted to maintainconsistency as much as possible.\\nTwo scoring methods are used to evaluate the algorithm’s out-\\nput. The ﬁrst score, P\\nr, is deﬁned to be the length of the longest\\noverlap between the output interval and a truth interval in the song5 10 15 20 25 30 35 40 45 50 55 6000.10.20.30.40.50.60.70.80.91\\nWindow Length (seconds)Passing Rate0.3\\n0.50.70.80.9\\nFigure 4: The chroma-based algorithm’s passing rate for Pp(solid\\nlines) and Pr(dotted lines) under various thresholds.\\ndivided by the total length of that truth interval. This score is effec-', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='tively a frame-level recall rate. An interval receives a sub-optimalP\\nrscore when the output does not contain all of the relevant cho-\\nrus. The second score, Pp, is deﬁned as the same overlap length\\ndivided by the length of the output interval. This score is effec-tively a measure of frame-level precision. P\\nperrors occur when\\nthe system includes selects portions of a song that are not con-tained in the chorus. Clearly, there is an inherent tradeoff between\\nthese scores. An output interval that is too long will most likely\\nhave a high P\\nrbut a low Pp; if the thumbnail is too short, the\\nsituation is reversed.\\nFigure 3 shows a comparison of the performance of our chroma-\\nbased algorithm versus the same algorithm using mel-frequency\\ncepstral coefﬁcients and also a random selection method. For each', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='method, the mean of each score over the entire database is plottedas a function of window size. The MFCCs are calculated at thesame rate as the chroma-based features; aside from the use of adifferent set of features, the algorithm is the same. The random\\nselection method calculates the mean score of 1000 uniformly dis-\\ntributed thumbnails for each song. From this ﬁgure, we can seethat our chroma-based implementation of the algorithm performssubstantially better than the random selection method and the al-\\ngorithm employing MFCCs for both scores and over all of the win-\\ndow lengths plotted here. Also, the expected tradeoff between P\\nr\\nand Ppwith window size is clearly shown. It is interesting to note\\nthat the intersection of all three sets of curves occurs around 23 sec-\\nonds, which is the mean length of truth intervals in our database.\\nThis should not be surprising; if the output selection has the samelength as the overlapping truth interval, P\\nrand Ppwill be identi-\\ncal.', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='cal.\\nAnother useful measure of performance is the fraction of the\\nsongs with a score exceeding some threshold, or the “passing rate.”\\nFigure 4 displays the chroma-based algorithm’s passing rates for\\nboth scores versus window length under various thresholds. Onceagain, the tradeoff between the two scores is evident, and the in-tersection of the two score curves for each threshold lies in the\\nvicinity of 23 seconds. This ﬁgure shows that we can obtain good\\npassing rates for one score without too great an effect on the other\\nif we choose a window size around 20 to 25 seconds.\\nIt is instructive to see when and why the system fails. The\\n21-24 October 2001, New Paltz, New York 17', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 2}),\n",
       " Document(page_content='most common failure occurs when the chorus or refrain is repeated,\\nbut there is some change, either in instrumentation or in the mu-\\nsical structure of the repeat. Failure in these cases occurs when\\nthe verses (or some other sections) of the song are more similarto one another than the modiﬁed repetitions of the chorus or re-frain. These cases violate our initial assumption of high correlationbetween instances of the chorus and indicate that this assumption\\nshould be relaxed. A less common error occurs when the repetition\\nof some “uninteresting” portion of the song has a high enough cor-relation enough to overshadow the repetitions of the chorus. Oneexample of this is the repetition of “Hello” in Nirvana’s Smells Like\\nTeen Spirit , which is selected by our algorithm as a good thumb-\\nnail. In other cases this may be an instrumental section, such asthe introduction of the song.\\n5. DISCUSSION', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='5. DISCUSSION\\nWe have shown that our algorithm for selecting audio thumbnailsoperates quite well on one database of popular music. Generally,the system fails when a song does not meet our initial assump-\\ntion that strongly repeated portions of a song correspond to the\\nchorus, refrain, or otherwise important part of a song. We can gen-eralize this cause of error and predict that the system will mostlikely perform poorly on types of music that do not have the sim-ple “verse-refrain” form often found in popular music. We have\\nalready argued that the structure of classical music is too compli-\\ncated to yield readily to this simple approach. Similarly, the im-provisational nature of jazz and blues, for instance, violates ouroriginal assumption as well. Thus we would not expect the system\\nto perform well in those cases.\\nThe most important conclusion that can be drawn from these\\nresults relates to the potential of chroma-based representations for', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='encoding musical structure. While the algorithm does perform bet-ter than chance when using MFCCs as features, the algorithm per-forms signiﬁcantly better when we employ our chroma-based fea-tures. This indicates that these chroma features are a much better\\nchoice for this application. MFCCs are a general purpose mea-\\nsure of the smoothed spectrum of an audio signal which primarilyrepresent the timbral aspects of the sound. Since we are seekingstructure in music, we would prefer to represent the elements of\\nthe music that provide its structure: notes, harmonic progressions,\\nand so on. These elements are encoded in the ﬁne spectral struc-ture of the audio signal and are not well represented by MFCCs.Our chroma-based representation retains and exploits much of this\\nﬁne structure.\\nWe have hypothesized that a chroma-based feature class can\\nbe used to capture harmonic relationships within a song. An ex-amination of the chroma features themselves indicates that they', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='do encode harmonic relationships to some extent. The features ex-hibit long-term patterns that are indicative the dominant harmoniesin a song and may be useful for determining musical key. Locally,\\none can identify changes that to map to harmonic progressions in\\nthe song. This suggests that a chroma-based feature class could beuseful for any application that requires structural analysis of musicbased on harmonic relationships.\\nThere are a number of interesting extensions that could be\\nmade to this simple method. First, the ability to optimize over win-dow size would be useful to mitigate the tradeoff between the two\\nsources of error. Further, performance would likely be improved\\nif the system could take advantage of more structural informationthan just the highest pairwise correlation. This extension couldalso form the basis of an extension to a full music-segmentationsystem based on structural content, which could be valuable for', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='musical form analysis. It is also possible that this method would\\nbe a useful technique for multimedia search and retrieval systems.\\n6. CONCLUSION\\nWe have presented a system which uses chroma-based represen-tations of sound to isolate repeated sections within popular musicfor the purpose of producing short, representative samples of en-\\ntire songs. Such a system has numerous applications, including\\nthe browsing of musical databases and multimedia search results.Perhaps more importantly, the success of this system serves to il-lustrate the potential of chroma-based representations for the struc-\\ntural analysis of musical content as an alternative to mel-frequency\\ncepstral coefﬁcients. This system provides a ﬁrst step towardsusing chroma-based representations as an important element ofmore sophisticated analysis systems, including segmentation and\\nsearch-and-retrieval.\\n7. ACKNOWLEDGEMENTS\\nThis material is based upon work supported, in part, by a Na-', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='tional Science Foundation Graduate Research Fellowship and bygrants from the National Science Foundation (IIS-0085945) andthe MusEn Project at the University of Michigan through the Of-\\nﬁce of the Vice President for Research. The authors would like\\nto thank members of the MusEn Project, William Birmingham,Bryan Pardo, Colin Meek, and Maureen Mellody, for their com-ments and contributions to this work. The authors would also like\\nto thank the anonymous reviewers of this paper for their helpful\\nsuggestions for improvements.\\n8. REFERENCES\\n[1] B. Logan and S. Chu, “Music summarization using key\\nphrases,” in International Conference on Acoustics, Speech\\nand Signal Processing , 2000.\\n[2] L. R. Rabiner and B.-H. Juang, Fundamentals of Speech\\nRecognition , Prentice-Hall, 1993.\\n[3] J. Foote, “Automatic audio segmentation using a measure of\\naudio novelty,” in Proceedings of IEEE International Confer-\\nence on Multimedia and Expo , 1999, vol. I, pp. 452–455.', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='[4] J. Foote, “Visualizing music and audio using self-similarity,”\\ninProceedings of ACM Multimedia ’99, Orlando, Florida ,\\nNovember 1999, pp. 77–80.\\n[5] G. H. Wakeﬁeld, “Mathematical representation of joint time-\\nchroma distributions,” in SPIE, Denver, Colorado , 1999.\\n[6] R. Shepard, “Circularity in judgements of relative pitch,”\\nJournal of the Acoustical Society of America , vol. 36, pp.\\n2346–2353, 1964.\\n[7] R. D. Patterson, “Spiral detection of periodicity and the spiral\\nform of musical scales,” in Psychology of Music , chapter 14,\\npp. 44–61. 1986.\\n[8] S. Dixon, “A lightweight multi-agent musical beat tracking\\nsystem,” in Proceedings of the AAAI Workshop on Artiﬁcial\\nIntelligence and Music: Towards Formal Models for Compo-sition, Performance and Analysis . 2000, AAAI Press.\\n18  \\nIEEE Workshop on Applications of Signal Processing to Audio and Acoustics 2001', metadata={'source': 'documents/pdf/BartW01-chorus.pdf', 'page': 3}),\n",
       " Document(page_content='See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/225860996\\nMusic Structu re Analysis from Acoustic Signals\\nChapt er · Januar y 2009\\nDOI: 10.1007/978-0-387-30441-0_21\\nCITATIONS\\n59READS\\n1,113\\n2 author s:\\nRoger B Dannenber g\\nCarne gie Mellon Univ ersity\\n265 PUBLICA TIONS \\xa0\\xa0\\xa05,202  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nMasat aka Got o\\nNational Instit ute of Adv anced Industrial Scienc e and T echnolog y\\n317 PUBLICA TIONS \\xa0\\xa0\\xa07,593  CITATIONS \\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll c ontent f ollo wing this p age was uplo aded b y Roger B Dannenber g on 21 May 2014.\\nThe user has r equest ed enhanc ement of the do wnlo aded file.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 0}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 1}),\n",
       " Document(page_content='- 1 - Music Structure Analysis from Acoustic Signals Roger B. Dannenberg and Masataka Goto  Abstract Music is full of structure, including sections, sequences of distinct musical textures, and the repetition of phrases or entire sections. The analysis of music audio relies upon feature vectors that convey information about music texture or pitch content. Texture generally refers to the average spectral shape and statistical fluctuation, often reflecting the set of sounding instruments, e.g. strings, vocal, or drums. Pitch content reflects melody and harmony, which is often independent of texture. Structure is found in several ways. Segment boundaries can be detected by observing marked changes in locally averaged texture. Similar sections of music can be detected by clustering segments with similar average textures. The repetition of a sequence of music often marks a logical segment. Repeated phrases and hierarchical structures can be discovered by finding similar sequences of feature', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 1}),\n",
       " Document(page_content='of feature vectors within a piece of music. Structure analysis can be used to construct music summaries and to assist music browsing. Introduction Probably everyone would agree that music has structure, but most of the interesting musical information that we perceive lies hidden below the complex surface of the audio signal. From this signal, human listeners perceive vocal and instrumental lines, orchestration, rhythm, harmony, bass lines, and other features. Unfortunately, music audio signals have resisted our attempts to extract this kind of information. Researchers are making progress, but so far, computers have not come near to human levels of performance in detecting notes, processing rhythms, or identifying instruments in a typical (polyphonic) music audio texture. On a longer time scale, listeners can hear structure including the chorus and verse in songs, sections in other types of music, repetition, and other patterns. One might think that without the reliable detection and', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 1}),\n",
       " Document(page_content='detection and identification of short-term features such as notes and their sources, that it would be impossible to deduce any information whatsoever about even higher levels of abstraction. Surprisingly, it is possible to automatically detect a great deal of information concerning music structure. For example, it is possible to label the structure of a song as AABA, meaning that opening material (the “A” part) is repeated once, then contrasting material (the “B” part) is played, and then the opening material is played again at the end. This structural description may be deduced from low-level audio signals. Consequently, a computer might locate the “chorus” of a song without having any representation of the melody or rhythm that characterizes the chorus. Underlying almost all work in this area is the concept that structure is induced by the repetition of similar material. This is in contrast to, say, speech recognition, where there is a common understanding of words, their structure,', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 1}),\n",
       " Document(page_content='their structure, and their meaning. A string of unique words can be understood using prior knowledge of the language. Music, however, has no language or dictionary (although there are certainly known forms and conventions). In general, structure can only arise in music through repetition or systematic transformations of some kind. Repetition implies there is some notion of similarity. Similarity can exist between two points in time (or at least two very short time intervals), similarity can exist between two sequences over longer time intervals, and similarity can exist between the longer-term statistical behaviors of acoustical features. Different approaches to similarity will be described. Similarity can be used to segment music: contiguous regions of similar music can be grouped together into segments. Segments can then be grouped into clusters. The segmentation of a musical work and the grouping of these segments into clusters is a form of analysis or “explanation” of the music.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 1}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='- 2 - Features and Similarity Measures A variety of approaches are used to measure similarity, but it should be clear that a direct comparison of the waveform data or individual samples will not be useful. Large differences in waveforms can be imperceptible, so we need to derive features of waveform data that are more perceptually meaningful and compare these features with an appropriate measure of similarity.  Feature Vectors for Spectrum, Texture, and Pitch Different features emphasize different aspects of the music. For example, mel-frequency cepstral coefficients (MFCCs) seem to work well when the general shape of the spectrum but not necessarily pitch information is important. MFCCs generally capture overall “texture” or timbral information (what instruments are playing in what general pitch range), but some pitch information is captured, and results depend upon the number of coefficients used as well as the underlying musical signal. When pitch is important, e.g. when searching', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='e.g. when searching for similar harmonic sequences, the chromagram is effective. The chromagram is based on the idea that tones separated by octaves have the same perceived value of chroma  (Shepard 1964). Just as we can describe the chroma aspect of pitch, the short term frequency spectrum can be restructured into the chroma spectrum by combining energy at different octaves into just one octave. The chroma vector is a discretized version of the chroma spectrum where energy is summed into 12 log-spaced divisions of the octave corresponding to pitch classes (C, C#, D, … B). By analogy to the spectrogram, the discrete chromagram is a sequence of chroma vectors. It should be noted that there are several variations of the chromagram. The computation typically begins with a short-term Fourier transform (STFT) which is used to compute the magnitude spectrum. There are different ways to “project” this onto the 12-element chroma vector. Each STFT bin can be mapped directly to the most', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='to the most appropriate chroma vector element (Bartsch and Wakefield 2001), or the STFT bin data can be interpolated or windowed to divide the bin value among two neighboring vector elements (Goto 2003a). Log magnitude values can be used to emphasize the presence of low-energy harmonics. Values can also be averaged, summed, or the vector can be computed to conserve the total energy. The chromagram can also be computed by using the Wavelet transform. Regardless of the exact details, the primary attraction of the chroma vector is that, by ignoring octaves, the vector is relatively insensitive to overall spectral energy distribution and thus to timbral variations. However, since fundamental frequencies and lower harmonics of tones feature prominently in the calculation of the chroma vector, it is quite sensitive to pitch class content, making it ideal for the detection of similar harmonic sequences in music. While MFCCs and chroma vectors can be calculated from a single short term', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='a single short term Fourier transform, features can also be obtained from longer sequences of spectral frames. Tzanetakis and Cook (1999) use means and variances of a variety of features in a one second window. The features include the spectral centroid, spectral rolloff, spectral flux, and RMS energy.  Peeters, La Burthe, and Rodet (2002) describe “dynamic” features, which model the variation of the short term spectrum over windows of about one second. In this approach, the audio signal is passed through a bank of Mel filters. The time-varying magnitudes of these filter outputs are each analyzed by a short term Fourier transform. The resulting set of features, the Fourier coefficients from each Mel filter output, is large, so a supervised learning scheme is used to find features that maximize the mutual information between feature values and hand-labeled music structures. Measures of Similarity Given a feature vector such as the MFCC or chroma vector, some measure of similarity is', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='of similarity is needed. One possibility is to compute the (dis)similarity using the Euclidean distance between feature vectors. Euclidean distance will be dependent upon feature magnitude, which is often a measure of the overall', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 2}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 3}),\n",
       " Document(page_content='- 3 - music signal energy. To avoid giving more weight to the louder moments of music, feature vectors can be normalized, for example, to a mean of zero and a standard deviation of one or to a maximum element of one. Alternatively, similarity can be measured using the scalar (dot) product of the feature vectors. This measure will be larger when feature vectors have a similar direction. As with Euclidean distance, the scalar product will also vary as a function of the overall magnitude of the feature vectors. If the dot product is normalized by the feature vector magnitudes, the result is equal to the cosine of the angle between the vectors. If the feature vectors are first normalized to have a mean of zero, the cosine angle is equivalent to the correlation, another measure that has been used with success. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) use a constant-Q transform (CQT), and found that CQT outperforms chroma and MFCC features using a cosine distance measure. They also', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 3}),\n",
       " Document(page_content='measure. They also introduce a “structure-based” distance measure that takes into account the harmonic structure of spectra to emphasize pitch similarity over timbral similarity, resulting in additional improvement in a music structure analysis task. Similarity can be calculated between individual feature vectors, as suggested above, but similarity can also be computed over a window of feature vectors. The measure suggested by Foote (1999) is vector correlation:  !\"=++•=10)(1),(wkkjkiwVVwjiS (1) where w is the window size. This measure is appropriate when feature vectors vary with time, forming significant temporal patterns. In some of the work that will be described below, the detection of temporal patterns is viewed as a processing step that takes place after the determination of similarity. Evaluation of Features and Similarity Measures Linear prediction coefficients (LPC) offer another low-dimensional approximation to spectral shape, and other encodings such as moments (centroid,', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 3}),\n",
       " Document(page_content='moments (centroid, standard deviation, skewness, etc.) are possible. Aucouturier and Sandler (2001) compare various approaches and representations. Their ultimate goal is to segment music according to texture, which they define as the combination of instruments that are playing together. This requires sensitivity to the general spectral shape, and insensitivity to the spectral details that vary according to pitch. They conclude that a vector of about 10 MFCCs is superior to LPC and discrete cepstrum coefficients (Galas and Rodet 1990).  On the other hand, Hu, Dannenberg, and Tzanetakis (2003) compare features for detecting similarity between acoustic and synthesized realizations of a single work of music. In this case, the goal is to ignore timbral differences between acoustic and synthetic instruments, but to achieve fine discrimination of pitches and harmonies. They conclude that the chroma vector is superior to pitch histograms and MFCCs.  Segmentation One approach to discovering', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 3}),\n",
       " Document(page_content='to discovering structure in music is to locate segments of similar musical material and the boundaries between them. Segmentation does not rely on classification or the discovery of higher order structure in music. However, one can envision using segmentation as a starting point for a number of more complicated tasks, including music summarization, music analysis, music search, and genre classification. Segmentation can also assist in audio browsing, a task that can be enhanced through some sort of visual summary of music and audio segments.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 3}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 4}),\n",
       " Document(page_content='- 4 - Segmentation Using Texture Change Tzanetakis and Cook (1999) perform segmentation as follows: Feature vectors Vi are computed as described above. A feature time differential, Δi, is defined as the Mahalanobis distance:  Δi = (Vi – Vi−1)TΣ-1(Vi – Vi−1) (2) where Σ is an estimate of the feature covariance matrix, calculated from the training data, and i is the frame number (time). This measure is related to the Euclidean distance but takes into account the variance and correlations among features. Next, the first order differences of the distance, Δi − Δi-1, are computed. A large difference indicates a sudden transition. Peaks are picked, beginning with the maximum. After a peak is selected, the peak and its neighborhood are zeroed to avoid picking another peak within the same neighborhood. Assuming the total number of segments is given a priori, the neighborhood is 20% of the average segment size. Additional peaks are selected and zeroed until the desired number of peaks (segment', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 4}),\n",
       " Document(page_content='of peaks (segment boundaries) has been obtained. Segmentation by Clustering Logan and Chu (2000) describe a clustering technique for discovering music structure. The goal is to label each frame of audio so that frames within similar sections of music will have the same labels. For example, all frames within all occurrences of the chorus should have the same label. This can be accomplished using bottom-up clustering to merge clusters that are similar. Initially, the feature vectors are divided into fixed-length contiguous segments and each segment receives a different label. The following clustering step is iterated: Calculate the mean µ and covariance Σ of the feature vectors within each cluster. Compute a modified Kullback Leibler (KL) distance between each pair of clusters, as described below. Find the pair of clusters with the minimum KL2 distance, and if this distance is below a threshold, combine the clusters. Repeat this step until no distance is below the threshold. The KL2', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 4}),\n",
       " Document(page_content='threshold. The KL2 distance between two Gaussian distributions A and B is given by:  );();(),(2ABKLBAKLBAKL+= (3)  !!\"#$$%&\\'+\\'()+\\'\\'+\\'\\'=BABAABBA11)(µµ (4) Segmentation and Hidden Markov Models Another approach to segmentation uses a hidden Markov model (HMM). In this approach, segments of music correspond to discrete states Q and segment transitions correspond to state changes. Time advances in discrete steps corresponding to feature vectors, and transitions from one state to the next are modeled by a probability distribution that depends only on the current state. This forms a Markov model that generates a sequence of states. Note that states are “hidden” because only feature vectors are observable. Another probability distribution, p(Vi | qi), models the generation of feature vector Vi from state qi. The left side of Figure 1 illustrates a 4-state ergodic Markov model, where arrows represent state transition probabilities. The right side of the figure illustrates the observation', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 4}),\n",
       " Document(page_content='the observation generation process, where arrows denote conditional probabilities between variables.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 4}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 5}),\n",
       " Document(page_content='- 5 - dabcstate sequence(hidden)feature vectors(observable)q1q2q3V1V2V3...P(q2|q1)P(V1|q1) Figure 1. Hidden Markov model with four hidden states a, b, c, and d. As shown, feature vectors depend only upon the current state, which depends only upon the previous state. The HMM has advantages for segmentation. In general, feature vectors do not indicate the current state (segment class) unambiguously, so when a single feature vector is observed, one cannot assume that it was generated by particular state. However, some features are more likely to occur in one state than another, so one can observe the trend of feature vectors, ignoring the unlikely outliers and guessing the state that is most consistent with the observations. If transitions are very unlikely, one may have to assume many outliers occur. On the other hand, if transitions are common and segments are short, one can change states rapidly to account for different feature vectors. The HMM formalism can determine the segmentation', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 5}),\n",
       " Document(page_content='the segmentation (the hidden state sequence) with the maximum likelihood given a set of transition probabilities and observations, thus the model can formalize the tradeoffs between minimizing transitions and matching features to states. Furthermore, HMM transition probabilities can be estimated from unlabeled training data, eliminating the need to guess transition probabilities manually. Aucouturier and Sandler (2001) model the observation probability distribution P(Vi|qj) as a mixture of Gaussian distributions over the feature space:  ),,()|(,,1,mjmjMmimjjiVcqVP!\"=#=µN (5) where N is a Gaussian probability density function with mean µi,m, covariance matrix Γj,m, and cj,m is a mixture coefficient. Here, i indexes time and j indexes state. They train the HMM using the Baum-Welch algorithm using the sequence vectors from the single song chosen for analysis. The Viterbi algorithm is then used to find the sequence of hidden states with the maximum likelihood, given the observed feature', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 5}),\n",
       " Document(page_content='observed feature vectors.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 5}),\n",
       " Document(page_content='012 Figure 2. Segmentation of 20 seconds of a song. State 0 is silence, State 1 is voice, accordion, and accompaniment, and State 2 is accordion and accompaniment. One potential drawback of this approach is that the HMM will segment the signal according to fine-grain changes in spectral content rather than long-term elements of musical form. For example, in one of Aucoturier and Sandler’s test cases (see Figure 2), the HMM segmentation appears to isolate individual words of a singer rather than divide the song according to verses and instrumental interludes. (Aucouturier, Pachet, and Sandler 2005) In other words, the segments can be quite short when there are rapid changes in the music. Although this might be the desired result, it seems likely that one could detect longer-term, higher-level music structure by averaging features over a longer time span or applying further processing to the state sequence obtained from an HMM.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 5}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 6}),\n",
       " Document(page_content='- 6 - Peeters, La Burthe, and Rodet (2002) approach the problem of clustering with a two-pass algorithm. Imagine a human listener hearing a piece of music for the first time. The range of variation of music features becomes apparent, and templates or classes of music are formed. In the second hearing, the structure of the music can be identified in terms of the previously identified templates. An automated system is inspired by this two-pass model. In the first pass, texture change indicates segment boundaries, and “potential” states are formed from the mean values of feature vectors within segments. In the second pass, potential states that are highly similar are merged by using the K-means algorithm. The resulting K states are called the “middle” states. Because they represent clusters with no regard for temporal contiguity, a hidden Markov model initialized with these “middle” states is then used to inhibit rapid inappropriate state transitions by penalizing them. The Baum-Welch', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 6}),\n",
       " Document(page_content='The Baum-Welch algorithm is used to train the model on the sequence of feature vectors from the song. Viterbi decoding is used to obtain a state sequence. Figure 3 shows the result of an analysis using this smoothing technique. HMM State Assignments0123050100150200250TimeState Figure 3. Classification of states in \"Head Over Feet\" from artist Alanis Morisette. (Adapted from Peeters, La Burthe, and Rodet, 2002).  The Similarity Matrix A concept used by many researchers is the similarity matrix. Given a sequence of feature vectors Vi and a measure of similarity S(i, j), one can simply view S(i, j) as a matrix. The matrix can be visualized using a grayscale image where black represents dissimilar vectors and white represents similar vectors. Shades of gray represent intermediate values. Since any vector is similar to itself, the diagonal of the similarity matrix will be white. Also, assuming the similarity measure is symmetric, the matrix will be symmetric about the diagonal. The', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 6}),\n",
       " Document(page_content='the diagonal. The interesting information in the matrix is in the patterns formed off the diagonal.  In very general terms, there are two interesting sorts of patterns that appear in the similarity matrix, depending on the nature of the features. The first of these appears when features correspond to relatively long-term textures. The second appears when features correspond to detailed short-term features such as pitch or harmony and where similar sequences of features can be observed. These two types of patterns are considered in the next two sections. Texture Patterns First, consider the case where features represent the general texture of the music, for example whether the music is primarily vocal, drum solo, or guitar solo. Figure 4 shows an idealized similarity matrix for this case. The white diagonal appears because feature vectors along the diagonal are identical. Notice that wherever there are similar textures, the matrix is lighter in color (more similar), so for example, all', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 6}),\n",
       " Document(page_content='so for example, all of the feature vectors for the vocals (V) are similar to one another, resulting in large light-colored square patterns both on and off the diagonal. Where two feature vectors correspond to different textures, for example drums and vocals, the matrix is dark.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 6}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 7 - DVGVVDGV\\nGG', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 7}),\n",
       " Document(page_content='Figure 4. An idealized similarity matrix for segments of drum (D), vocal (V), and guitar (G) texture. Notice that along the diagonal, a checkerboard pattern appears at segment boundaries, with darker regions to the upper left and lower right, and lighter regions to the lower left and upper right. Foote (2000) proposes the correlation of the similarity matrix S with a kernel based on this checkerboard pattern in order to detect segment boundaries. The general form of the kernel is:  !!!!\"#$$$$%&\\'\\'\\'\\'\\'\\'\\'\\'=1111111111111111C (6) (Note that in Equation 6, row numbers increase in the downward direction whereas in the similarity matrix images, the row number increases in the upward direction. Therefore the diagonal in Equation 6 runs from upper left to lower right.) The kernel image in Figure 5 represents a larger checkerboard pattern with radial smoothing. The correlation N(i) of this kernel along the diagonal of a similarity matrix S can be considered to be a measure of novelty (Foote', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 7}),\n",
       " Document(page_content='of novelty (Foote 2000):  !!\"=\"=++=2/2/2/2/  ),(),()(LLmLLnnimiSnmCiN (7) A graph of N(i) for the similarity matrix in Figure 4 is shown in Figure 5. A peak occurs at each transition because transition boundaries have the highest correlation to the checkerboard pattern.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 7}),\n",
       " Document(page_content='Checkerboard Kernel  \\n  Checkerboard Kernel with  Radial Smoothing Correlation Along Diagonal\\n00.20.40.60.81\\n0102030405060Frame OffsetCorrelation Figure 5. The correlation of the kernel shown at lower left with a similarity matrix. Cooper and Foote (2003) extend this technique for finding segment boundaries with a statistical method for clustering segments.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 7}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 8}),\n",
       " Document(page_content='- 8 - Repeating Sequence Patterns While the texture patterns described above are most useful for detecting transitions between segments, the second kind of pattern can be used to discover repetition within a song. For these patterns to appear, it is important that features reflect short-term changes. Generally, features should vary significantly with changes in the pitch of a melody or with changes in harmony. If this condition is satisfied, then there will not be great similarity within a segment, and there will not be a clear pattern of light-colored squares as seen in Figure 4. However, if a segment of music repeats with an offset of j, then S(i, i) will equal S(i, i+j), generating a diagonal line segment at an offset of j from the central diagonal. This is illustrated schematically in Figure 6, where it is assumed that the vocal sections (V) constitute three repetitions of very similar music, whereas the two guitar sections (G) are not so similar. Notice that each non-central', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 8}),\n",
       " Document(page_content='each non-central diagonal line segment indicates the starting times and the duration of two similar sequences of features. Also, notice that since each pair of similar sequences is represented by two diagonal line segments, there are a total of six (6) off-central line segments in Figure 6.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 8}),\n",
       " Document(page_content='DVGVVDGV\\nGG\\nVV\\n Figure 6. When sections of music are repeated, a pattern of diagonal line segments is generated. Although not shown in Figure 6, the similarity matrix can also illustrate hierarchical relationships. For example, if each vocal section (V) consists of a phrase that is repeated, the similarity matrix would look like the one in Figure 7.  \\nDVGVVDGV\\nGG\\nVV\\n Figure 7. The vocal segments (V) in this similarity matrix contain a repetition, generating additional pattern that is characteristic of music structure.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 8}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 9 - Figure 8 illustrates both texture patterns and repeated sequence patterns from the song “Day Tripper” by the Beatles. The bridge is displayed, starting with three repetitions of a two-measure guitar phrase in the first 11 seconds, followed by six measures of vocals. Notice how a checkerboard pattern appears due to the timbral self-similarity of the guitar section (0 to 11s) and the vocal section (11 to 21s). Finer structure is also visible. A repeated sequence pattern appears within the guitar section as parallel diagonal lines. This figure uses the power spectrum below 5.5kHz as the feature vector, and uses the cosine of the angle between vectors as a measure of similarity.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 9}),\n",
       " Document(page_content='Figure 8. Similarity matrix using spectral features from the bridge of \"Day Tripper\" by the Beatles. The Time-Lag Matrix When the goal is to find repeating sequence patterns, it is sometimes simpler to change coordinate systems so that patterns appear as horizontal or vertical lines. The time-lag matrix r is defined by:   r(t, l) = S(t, t−l), where t−l ≥ 0 (8) Thus, if there is repetition, there will be a sequence of similar frames with a constant lag. Since lag is represented by the vertical axis, a constant lag implies a horizontal line. The time-lag version of Figure 7 is shown in Figure 9. Only the lines representing similar sequences are shown, and the grayscale has been reversed, so that similarity is indicated by black lines.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 9}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 10 - r(t, l)l (lag)', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 10}),\n",
       " Document(page_content='t (time)DVGVGV Figure 9. Time-lag matrix representation of the similarity matrix in Figure 7. Finding Repeating Sequences Of course, with audio data obtained from real audio recordings, the similarity or time-lag matrix will be full of noise and ambiguity arising from spurious similarity between different frames. Furthermore, repetitions in music are rarely exact; variation of melodic, harmonic, and rhythmic themes is an essential characteristic of music. In order to automate the discovery of musical structure, algorithms must be developed to identify the structure that lies within the similarity matrix. Melodic Sequence Matching One way to find repetition is to transcribe the melody and perform matching on the resulting symbolic transcription. While extracting the melody from a polyphonic recording (Goto 2004) is very difficult in general, an approximate transcription from an instrumental recording or from a monophonic (melody only) recording is relatively easy. Dannenberg (2002)', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 10}),\n",
       " Document(page_content='Dannenberg (2002) describes a simple transcription system based on the enhanced autocorrelation algorithm (Tolonen and Karjalainen 2000) applied to a ballad recorded by John Coltrane. The transcription results in a quantized integer pitch value pi and real inter-onset interval di for each note (inter-onset intervals are typically preferred over note duration in music processing). This sequence is processed as follows: 1) First, a similarity matrix is constructed where rows and columns correspond to notes. This differs from the similarity matrix described above where rows and columns correspond to feature vectors with a fixed duration. 2) Each cell of the similarity matrix S(i, j) represents the duration of similar melodic sequences starting at notes i and j. A simple “greedy” algorithm is used to match these two sequences. If note i does not match note j, S(i, j) = 0. 3) Simplify the matrix by removing redundant entries. If a sequence beginning at i matches one at j, then there should', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 10}),\n",
       " Document(page_content='then there should be another match at i+1 and j+1. To simplify the matrix, find the submatrix S(i:u, j:v) where the matching sequences at i and j end at u and v. Zero every entry in the submatrix except S(i, j). Also, zero all entries for matching sequences of length 1. 4) Now, any non-zero entry in S represents a pair of matching sequences. By scanning across rows of S we can locate all similar sequences. Sequences are clustered: the first non-zero element in a row represents a cluster of two sequences. Any other non-zero entry in the row that roughly', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 10}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='- 11 - matches the durations of the clustered sequences is added to the cluster. After scanning the row, all pair-wise matches are zeroed so they will not be considered again. The result of this step is a set of clusters of similar melodic segments. Because repetitions in the music are not exact, there can be considerable overlap between clusters. It is possible for a long segment to be repeated exactly at one offset, and for a portion of that same segment to be repeated several times at other offsets. It may be desirable to simplify the analysis by labeling each note with a particular cluster. This simplification is described in the next section. Simplification or Music Explanation The goal of the simplification step is to produce one possible set of labels for notes. Ideally, the labels should offer a simple “explanation” of the music that highlights repetition within the music. The AABA structure common in songs is a typical explanation. In general, longer sequences of notes are', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='of notes are preferable because they explain more, but when sequences are too long, interesting substructure may be lost. For example, the structure AABAAABA could also be represented as AABA repeated, i.e. the structure could be labeled AA, but most theorists would consider this to be a poor explanation. Hierarchical explanations offer a solution, but there is no formal notion as yet of the optimal simplification or explanation. Dannenberg uses a “greedy” algorithm to produce reasonable explanations from first note to last. (Dannenberg and Hu 2002) Notes are initially unlabeled. As each unlabeled note is encountered, search the clusters from the previous section to find one that includes the unlabeled note. If a cluster is found, allocate a new label, e.g. “A”, and label every note included in the cluster accordingly. Continue labeling with the next unlabeled note until all notes are processed. Figure 10 illustrates output from this process. Notice that the program discovered', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='program discovered substructure within what would normally be considered the “bridge” or the B part, but this substructure is “real” in the sense that one can see it and hear it. The gap in the middle of the piece is a piano solo where transcription failed. Notice that the program correctly determines that the saxophone enters on the bridge (the B part) after the piano solo. The program also identifies the repeated 2-measure phrase at the end. It fails to notice the structure of ascending pitches at the very end because, while this is a clear musical gesture, it is not based on the repetition of a note sequence.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='Figure 10. A computer analysis of \"Naima\" by John Coltrane. The automatic transcription appears as a \"piano roll\" at the top, the computer analysis appears as shaded bars, where similar shading indicates similar sequences, and conventional labels appear at the bottom. Finding Similar Sequences in the Similarity Matrix Typically, transcription of a music signal into a sequence of notes is not possible, so similar sequences must be detected as patterns in the similarity or time-lag matrix. For example, Bartsch and Wakefield (2001) filter along diagonals of a similarity matrix to detect similarity. This assumes nearly constant tempo, but that is a good assumption for the popular music used in their study. Their objective was not to identify the beginnings and endings of repeating sequences but to find the chorus of a popular song for use as an “audio thumbnail” or summary. The thumbnail is selected as the maximum element of the filtered similarity matrix, with the additional constraints', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='constraints that the lag is at least one tenth of the length of the song and the thumbnail does not appear in the last quarter of the song. Peeters and Rodet (2003) suggest using a 2D structuring filter on the lag matrix to detect similar sequences. Their filter counts the number of values in the neighborhood to the left and right of a point that', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 11}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 12}),\n",
       " Document(page_content='- 12 - are above a threshold. To allow for slight changes in tempo, which results in lines that are not perfectly horizontal, neighbor cells above and below are also considered. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) suggest erosion and dilation operations on the lag matrix to enhance and detect significant similar sequences. Dannenberg and Hu (2003) use a discrete algorithm to find similar sequences and is based on the idea that a path from cell to cell through the similarity matrix specifies an alignment between two subsequences of the feature vectors. If the path goes through S(i, j), then vector i is aligned with j. This suggests using a dynamic time warping (DTW) algorithm (Rabiner and Juang 1993), and the actual algorithm is related to DTW.  The goal is to find alignment paths that maximize the average similarity of the aligned features. A partial or complete path P is defined as a set of pairs of locations and is rated by the average similarity along the path:', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 12}),\n",
       " Document(page_content='along the path:  !\"=PjijiSPPq),(),(1)( (9) where |P| is the path length using Euclidean distance. Paths are extended as long as the rating remains above a threshold. Paths are constrained to move up one cell, right one cell, or diagonally to the upper right as shown in Figure 11 (and adopting the orientation of the similarity matrix visualizations where time increases vertically and to the right). Therefore, every point that is on a path can be reached from below, from the left, or from the lower left. Each cell (i,j) of an array is computed by looking at the cell below, left, and below left to find the (previously calculated) best path (highest q(P)) passing through those cells. Three new ratings of r are computed by extending each of the three paths to include (i,j). The path with the highest rating is remembered as the one passing through (i,j).  ijj+1i+1 Figure 11. Extending a path from S(i, j). Because cells depend on previously computed values to the lower left, cells are', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 12}),\n",
       " Document(page_content='left, cells are computed along diagonals of constant i+j, from lower left to upper right (increasing i+j). When no path has a rating above some fixed threshold, the path ends. A path may begin wherever S(i, j) is above threshold and no previous paths exist to be extended. Forming Clusters After alignment paths are found, they are grouped into clusters. So if sequence A aligns to sequence B, and sequence A also aligns to sequence C, then A, B, and C should be grouped in a single cluster. Unfortunately, it is unlikely that the alignments of A to B and A to C use exactly the same frames. It is more likely that A aligns to B and A′ aligns to C, where A and A′ are mostly overlapping. This can be handled simply by considering A to equal A′ when they start and end within some fraction of their total length, for example within 10 percent. Once clusters are formed, further simplification and explanation steps can be performed as described above. Isolating Line Segments from the Time-Lag Matrix', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 12}),\n",
       " Document(page_content='the Time-Lag Matrix If nearly constant tempo can be assumed, the alignment path is highly constrained and the alignment path approach may not work well. Taking advantage of the fact that similar sequences are represented by', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 12}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 13}),\n",
       " Document(page_content='- 13 - horizontal lines in the time-lag matrix, Goto (2003a) describes an alternative approach to detecting music structure. In this work, the time-lag matrix is first normalized by subtracting a local mean value while emphasizing horizontal lines. In more detail, given a point r(t,l) in the time-lag matrix, six-directional local mean values along the right, left, upper, lower, upper-right, and lower-left directions starting from r(t,l) are calculated, and the maximum and minimum are obtained. If the local mean along the right or left direction takes the maximum, r(t,l) is considered a part of a horizontal line and emphasized by subtracting the minimum from r(t,l).  Otherwise, r(t,l) is considered a noise and suppressed by subtracting the maximum from r(t,l); noises tend to appear as lines along the upper, lower, upper-right, and lower-left directions. Then, a summary is constructed by integrating over time:  !!dltlrltRtlall\"#=),(),( (10) Rall is then smoothed by a moving average', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 13}),\n",
       " Document(page_content='by a moving average filter along the lag. The result is sketched in Figure 12. Rall is used to decide which lag values should be considered when searching for line segments in the time-lag matrix. A thresholding scheme based on a discriminant criterion is used. The threshold is automatically set to maximize the following between-class variance of the two classes established by the threshold:  221212)(µµ!!\"#=B (11) where ω1 and ω2 are the probabilities of class occurrence (the fraction of peaks in each class), and µ1 and µ2 are the means of peak heights in each class. Each peak above threshold determines a lag value, lp. For each peak, the one-dimensional function r(τ, lp) is searched over lp ≤ τ ≤ t. A smoothing operation is applied to this function and the discriminant criterion of Equation 11 is again used to set the threshold. The result is the beginning and ending points of line segments that indicate repeated sections of music.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 13}),\n",
       " Document(page_content='Rall(t, l)r(t, l)l (lag)\\nt (time)DVGVGV Figure 12. The summary Rall(t, l) indicates the possibility that there are similar segments at a lag of l. Modulation Detection A common technique in pop music when repeating a chorus is to change the key, typically modulating upward by half steps. (Note that “modulation” in music is not related to amplitude modulation or frequency modulation in the signal processing sense.) Since modulation changes all the pitches, it is unlikely that a feature vector that is sensitive to pitch sequences could detect any similarity between musical passages in different keys. To a first approximation, a modulation in music corresponds to', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 13}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='- 14 - frequency scaling, as if changing the speed of a vinyl record turntable or changing the sample rate of a digital recording. On a logarithmic frequency scale, modulation is simply an offset, and when the scale is circular as with pitch classes and chroma, modulation is a rotation. To rotate a vector by ζ, the value of the ith feature is moved to become feature (i + ζ) mod 12. One would expect the chroma vectors for a modulated passage of music to be quite similar to a rotation of the chroma vectors of the unmodulated version. Goto (2003a) exploits this property of the chroma vector by extending the time-lag matrix to incorporate chroma vector rotation by a transposition amount ζ. Denoting !tV as a transposed (rotated) version of a chroma vector tV, rζ(t, l) is the similarity between !tV and the untransposed vector 0ltV!. Since we cannot assume the number of semitones at the modulation in general, the line segment detection is performed on each of 12 versions of rζ(t, l)', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='of rζ(t, l) corresponding to the 12 possible transpositions (this usually does not increase harmful false matches). The segments from all 12 versions are combined to form the set of repeated sections of music, and the transposition information can be saved to form a more complete explanation of the music structure. Chorus Selection after Grouping Line Segments Since each line segment indicates just a pair of repeated contiguous segments, it is necessary to organize into a cluster the line segments that have mostly overlapping frames. When a segment is repeated n times (n ≥ 3), the number of line segments to be grouped in a cluster should theoretically be n(n-1)/2 in the time-lag matrix. Aiming to exhaustively detect all the repeated segments (choruses) appearing in a song, Goto (2003a) describes an algorithm that redetects missing (hidden) line segments to be grouped by top-down processing using information on other detected line segments. The algorithm also appropriately adjusts the', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='adjusts the start and end times of line segments in each cluster because they are sometimes inconsistent in the bottom-up line-segment detection. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) describe another approach to obtain the best overall combination of segment similarity and duration by adjusting segment boundaries. A cluster corresponding to the chorus can be selected from those clusters. In general, a cluster that has many and long segments tends to be the chorus. In addition to this property, Goto (2003a) uses heuristic rules to select the chorus with a focus on popular music; for example, when a segment has half-length repeated sub-segments, it is likely to be the chorus. The choruslikeness (chorus possibility) of each cluster is computed by taking these rules into account, and the cluster that maximizes the choruslikeness is finally selected. Texture Sequences Detecting repeating patterns in the similarity matrix is equivalent to finding sequences of similar feature', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='of similar feature vectors. An alternative is to find sequences of similar texture classes. Aucouturier and Sandler (2002) perform a segmentation using hidden Markov models as described earlier. The result is a “texture score,” a sequence of states, e.g. 11222112200, in which patterns can be discovered. They explore two methods for detecting diagonal lines in the similarity matrix. The first is kernel convolution, similar to the filter method of Bartsch and Wakefield (2001). The second uses the Hough Transform (Leavers 1992), a common technique for detecting lines in images. The Hough Transform uses the familiar equation for a line: y = mx+b. A line passing through the point (x,y) must obey the equation b = −mx+y, which forms a line in the (m,b) space. A series of points along the line y = m0x+b0 can be transformed to a series of lines in (m,b) space that all intersect at (m0,b0). Thus, the problem becomes one of finding the intersection of lines. This can be accomplished, for', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='accomplished, for example, by making a sampled two-dimensional image of the (m,b) space and searching for local maxima. It appears that the Hough Transform could be used to find patterns in the similarity matrix as well as in the “texture score” representation.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 14}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='- 15 - One of the interesting features of the texture score representation is that it ignores pitch to a large extent. Thus, music segments that are similar in rhythm and instrumentation can be detected even if the pitches do not match. For example, “Happy Birthday” contains 4 phrases of 6 or 7 notes. There are obvious parallels between these phrases, yet they contain 4 distinct pitch sequences. It seems likely that pitch sequences, texture sequences, rhythmic sequences, and other feature sequences can provide complementary views that will facilitate structure analysis in future systems. Music Summary Browsing images or text is facilitated by the fact that people can shift their gaze from one place to another. The amount of material that is skipped can be controlled by the viewer, and in some cases, the viewer can make a quick scan to search for a particular image or to read headlines. Music, on the other hand, exists in time rather than space. Listeners cannot time-travel to scan a', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='to scan a music performance, or experience time more quickly to search for musical “headlines.” At best, one can skip songs or use fast-forward controls with recorded music, but even this is confusing and time-consuming. One application of music structure analysis is to enable the construction of musical “summaries” that give a short overview of the main elements of a musical work. Summaries can help people search for a particular piece of music they know or locate unfamiliar music they might like to hear in full. By analogy to low-resolution versions of images often used to save space or bandwidth, summaries of music are sometimes called “music thumbnails.” Cooper and Foote describe a simple criterion for a music summary of length L: the summary should be maximally similar to the whole. In other words, a summary can be rated by summing the similarity between each feature vector in the summary with each feature vector in the complete work. The rating for the summary beginning at', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='beginning at feature vector i is:  !!+===LiimNnLnmSNLiQ1),(1)( (12) The best summary is then the one starting at the value of i the maximizes QL(i). The formula can be extended by weighting S(m,n) to emphasize earlier or louder sections of the song. Other approaches to summary construction are outlined by Peeters, La Burthe, and Rodet (2002). Assume that music has been segmented using one of the techniques described above, resulting in three classes or labels A, B, and C. Some of the interesting approaches to musical summary are: • use the most common class, which in popular music is often the chorus. Some research specifically aims to determine the chorus as described earlier. (Bartsch and Wakefield 2001; Goto 2003a); • use a sample of music from each class, i.e A, B, C. • use examples of each class transition, i.e. A→B, B→A, A→C. In all cases, audio segments are extracted from the original music recording. Unfortunately, artificially and automatically generated transitions can be', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='transitions can be jarring to listeners. Music structure analysis can help to pick logical points for transitions. In particular, a cut from one phrase of music to a repetition of that phrase can be inaudible. When a cut must be made to a very different texture, it is generally best to make the cut at an existing point of strong textural change. In most music, tempo and meter create a framework that is important for listening. Cuts that jump from the end of one measure to the beginning of another preserve the short-term metrical structure of the original music and help listeners grasp the harmonic and melodic structure more easily. Segments that last 2, 4, or 8 measures (or some duration that relates to the music structure) are more likely to seem “logical” and less disruptive. Thus, music structure analysis is not only important to determine what sections of music to include in a summary, but also to organize those sections in a way that is “musical” and easy for the listener to', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='for the listener to comprehend.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 15}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005 \\n- 16 - An alternative to the construction of “music thumbnails” is to provide a “smart” interface that facilitates manual browsing of entire songs. The SmartMusicKIOSK music listening station (Goto 2003b) displays a time-line with the results of an automatic music structure analysis. In addition to the common stop, pause, play, rewind, and fast forward controls, the SmartMusicKIOSK has controls labeled “next chorus,” “next section,” and “prev section.” (See Figure 13.) These content-based controls allow users to skim rapidly through music and give a graphical overview of the entire music structure, which can be understood without listening to the entire song.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 16}),\n",
       " Document(page_content='Figure 13. The SmartMusicKIOSK user interface showing music structure and structure-related controls. Evaluation Most research in this area has been exploratory, with no means to evaluate whether computer-generated structures and segments are “correct.” In most cases, it is interesting simply to explore what types of structures can be uncovered and what methods can be used. Quantitative evaluations will become more important as problems are better understood and when competing methods need to be compared. Tzanetakis and Cook conducted a pilot study (1999) to compare their automatic segmentation with human segmentation. They found most human subjects agreed on more than half of the segments, and their machine segmentation found more than half of the segments that humans agreed upon. Bartsch and Wakefield (2001) hand-selected “true audio thumbnails” from 93 popular songs and measured “recall,” the fraction of true frames labeled by their program as the chorus, and “precision,” the', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 16}),\n",
       " Document(page_content='“precision,” the fraction of labeled chorus frames that  are true frames. With the chorus length set to around 20 to 25 seconds, the average recall and precision is about 70%, compared to about 30% for a chorus interval selected at random. Goto (Goto 2003a) also used hand-labeled choruses in 100 popular songs from the RWC Music Database, a source that enables researchers to work with common test data. (Goto et al. 2002) Goto judged the system output to be correct if the F-measure was more than 0.75. The F-measure is the harmonic mean of recall rate (R) and precision rate (P): F-measure = 2RP/(R+P). The system dealt correctly with 80 out of 100 songs. Evaluating music structure descriptions is difficult. Structure exists at many levels and often exhibits hierarchy. The structure intended by the composer and perhaps determined by a music theorist may not correspond to the perception of the typical listener. Nevertheless, one can ask human subjects to identify pattern and structure in', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 16}),\n",
       " Document(page_content='and structure in music, look for consistency between subjects, and then compare human descriptions to machine descriptions of music. One can also evaluate the impact of music structure detection upon tasks such a browsing, as in SmartMusicKIOSK (Goto 2003b).', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 16}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 17}),\n",
       " Document(page_content='- 17 - Summary and Conclusions Knowledge of musical structure can be used to construct music summaries, assist with music classification, provide high-level interfaces for music browsing, and offer high-level top-down guidance for further analysis. Automatic analysis of music structure is one source of music meta-data, which is important for digital music libraries.  High-level music structure is generally represented by partitioning the music into segments. Sometimes, segments are labeled to indicate similarity to other segments. There are two main principles used to detect high-level music structure. First, segment boundaries tend to occur when there is a substantial change in musical texture. In other words, this is where the music on either side of the boundary is self-similar, but the two regions differ from each other. Secondly, segments can be located by detecting patterns of repetition within a musical work. It should be noted that the music signal, viewed as a time-domain', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 17}),\n",
       " Document(page_content='as a time-domain waveform, is not directly useful for analysis because repetition in music is never exact enough to reproduce phase and amplitude relationships. Therefore, the signal is processed to obtain features that capture useful and more-or-less invariant properties. In the case of texture analysis, features should capture the overall spectral shape and be relatively insensitive to specific pitches. Low-order MFCCs are often used to measure texture similarity. To detect music repetition, features should capture changes in pitch and harmony, ignoring texture which may change from one repetition to the next. The chroma vector is often used in this case. The similarity matrix results from a comparison of all feature vector pairs. The similarity matrix offers an interesting visualization of music, and it has inspired the application of various image-processing techniques to detect music structure. Computing the correlation with a “checkerboard” kernel is one method for detecting', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 17}),\n",
       " Document(page_content='for detecting texture boundaries. Using filters to detect diagonal lines is one method for detecting repetition. Detecting segment boundaries or music repetition generates individual segments or pairs of segments. Further processing can be used to merge segments into clusters. Hidden Markov models, where each hidden state corresponds to a distinct texture, have been applied to this problem. When music is analyzed using repetitions, the structure can be hierarchical, and the structure is often ambiguous. Standard clustering algorithms assume a set of distinct, fixed items, but with music analysis, the items to be clustered are possibly overlapping segments whose start and end times might be adjustable. Music structure analysis is a rapidly-evolving field of study. Future work will likely explore the integration of existing techniques, combining texture-based with repetition-based segmentation. More sophisticated features including music transcription will offer alternative', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 17}),\n",
       " Document(page_content='offer alternative representations for analysis. Finally, there is the possibility to detect richer structures, including hierarchical patterns of repetition, rhythmic motives, harmonic progressions and key changes, and melodic phrases related by transposition. Acknowledgments The authors wish to thank Jonathan Foote for Figure 8. Jean-Julien Aucotourier, Mark Bartsch, Jonathan Foote, Geoffroy Peeters, Ning Hu, Xavier Rodet, Mark Sandler, George Tzanetakis, and Greg Wakefield have made contributions through their work, discussions, and correspondence. References  Aucouturier, J.-J., F. Pachet, and M. Sandler. 2005. \"\"The Way It Sounds\": Timbre Models for Structural Analysis and Retrieval of Music Signals.\" IEEE Transactions on Multimedia, (to appear).  Aucouturier, J.-J., and M. Sandler. 2001. \"Segmentation of Musical Signals Using Hidden Markov Models.\" In Proceedings of the 110th Convention of the Audio Engineering Society. Audio Engineering Society.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 17}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 18}),\n",
       " Document(page_content='- 18 - Aucouturier, J.-J., and M. Sandler. 2002. \"Finding Repeating Patterns in Acoustic Musical Signals: Applications for Audio Thumbnailing.\" In AES22 International Conference on Virtual, Synthetic and Entertainment Audio. Audio Engineering Society, pp. 412-421. Bartsch, M., and G. H. Wakefield. 2001. \"To Catch a Chorus: Using Chroma-Based Representations For Audio Thumbnailing.\" In Proceedings of the Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE, pp. 15-18. Cooper, M., and J. Foote. 2003. \"Summarizing Popular Music via Structural Similarity Analysis.\" In 2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE, pp. 127-130. Dannenberg, R. B. 2002. \"Listening to \"Naima\": An Automated Structural Analysis from Recorded Audio.\" In Proceedings of the 2002 International Computer Music Conference (ICMC 2002). San Francisco: International Computer Music Association, pp. 28-34. Dannenberg, R. B.,', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 18}),\n",
       " Document(page_content='Dannenberg, R. B., and N. Hu. 2002. Discovering Musical Structure in Audio Recordings. In Anagnostopoulou, C., et al. eds. Music and Artificial Intelligence, Second International Conference, ICMAI 2002. Berlin: Springer-Verlag, pp. 43-57 Dannenberg, R. B., and N. Hu. 2003. \"Pattern Discovery Techniques for Music Audio.\" Journal of New Music Research, 32(2), 153-164.  Foote, J. 1999. \"Visualizing Music and Audio Using Self-Similarity.\" In Proceedings of ACM Multimedia \\'99. New York: Association for Computing Machinery, pp. 77-80. Foote, J. 2000. \"Automatic Audio Segmentation Using a Measure of Audio Novelty.\" In Proceedings of the International Conference on Multimedia and Expo (ICME 2000). New York: IEEE, pp. 452-455. Galas, T., and X. Rodet. 1990. \"An Improved Cepstral Method for Deconvolution of Source-Filter Systems with Discrete Cepstral: Application to Musical Sounds.\" In 1990 International Computer Music Conference (ICMC 1990). San Francisco: International Computer Music', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 18}),\n",
       " Document(page_content='Computer Music Association, pp. 82-84. Goto, M. 2003a. \"A Chorus-Section Detecting Method for Musical Audio Signals.\" In Proceedings of the 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing Proceedings (ICASSP 2003). New York: IEEE, pp. V-437-440. Goto, M. 2003b. \"SmartMusicKIOSK: Music Listening Station with Chorus-Search Function.\" In Proceedings of the 16th Annual ACM Symposium on User Interface Software and Technology (UIST 2003). New York: Association for Computing Machinery, pp. 31-40. Goto, M. 2004. \"A Real-time Music Scene Description System: Predominatn-F0 Estimation for Detecting Melody and Bass Lines in Real-world Audio Signals.\" Speech Communication (ISCA Journal), 43(4), 311-329.  Goto, M., T. Nishimura, H. Hashiguchi, and R. Oka. 2002. \"RWC Music Database: Popular, Classical, and Jazz Music Databases.\" In ISMIR 2002 Conference Proceedings. Paris: IRCAM, pp. 287-288. Hu, N., R. B. Dannenberg, and G. Tzanetakis. 2003. \"Polyphonic Audio Matching', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 18}),\n",
       " Document(page_content='Audio Matching and Alignment for Music Retrieval.\" In 2003 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE, pp. 185-188. Leavers, V. F. 1992. Shape Detection in Computer Vision Using the Hough Transform. Berlin: Springer-Verlag. Logan, B., and S. Chu. 2000. \"Music Summarization Using Key Phrases.\" In Proceedings of the 2000 IEEE International Conference on Acoustics, Speech, and Signal Processing Proceedings (ICASSP 2000). New York: IEEE, pp. II-749-752. Lu, L., M. Wang, and H.-J. Zhang. 2004. \"Repeating Pattern Discovery and Structure Analysis from Acoustic Music Data.\" In Proceedings of the 6th ACM SIGMM International Workshop on Multimedia Information Retrieval. New York: Association for Computing Machinery, pp. 275-282.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 18}),\n",
       " Document(page_content='R. Dannenberg and M. Goto Music Structure 16 April 2005', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 19}),\n",
       " Document(page_content='- 19 - Peeters, G., A. L. Burthe, and X. Rodet. 2002. \"Toward Automatic Audio Summary Generation from Signal Analysis.\" In ISMIR 2002 Conference Proceedings. Paris: IRCAM, pp. 94-100. Peeters, G., and X. Rodet. 2003. \"Signal-based Music Structure Discovery for Music Audio Summary Generation.\" In Proceedings of the 2003 International Computer Music Conference (ICMC 2003). San Francisco: International Computer Music Association, pp. 15-22. Rabiner, L., and B.-H. Juang. 1993. Fundamentals of Speech Recognition. Englewood Cliffs, NJ: Prentice Hall. Shepard, R. 1964. \"Circularity in Judgements of Relative Pitch.\" Journal of the Acoustical Society of America, 36(12), 2346-2353.  Tolonen, T., and M. Karjalainen. 2000. \"A computationally efficient multi-pitch analysis model.\" IEEE Transactions on Speech and Audio Processing, 8(6), 708-716.  Tzanetakis, G., and P. Cook. 1999. \"Multifeature Audio Segmentation for Browsing and Annotation.\" In Proceedings of the Workshop on Applications of Signal', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 19}),\n",
       " Document(page_content='of Signal Processing to Audio and Acoustics (WASPAA). New York: IEEE.', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 19}),\n",
       " Document(page_content='View publication stats', metadata={'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf', 'page': 19})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_docs(documents,chunk_size=1000,chunk_overlap=20):\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "  docs = text_splitter.split_documents(documents)\n",
    "  return docs\n",
    "\n",
    "docs = split_docs(pdf_documents)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "144 chunks from 29 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceTransformer originates from Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas/.pyenv/versions/3.10.6/envs/knowledge_QA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma DB embedding store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2b8f19150>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = Chroma.from_documents(docs, embeddings)\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Figure 8. Similarity matrix using spectral features from the bridge of \"Day Tripper\" by the Beatles. The Time-Lag Matrix When the goal is to find repeating sequence patterns, it is sometimes simpler to change coordinate systems so that patterns appear as horizontal or vertical lines. The time-lag matrix r is defined by:   r(t, l) = S(t, t−l), where t−l ≥ 0 (8) Thus, if there is repetition, there will be a sequence of similar frames with a constant lag. Since lag is represented by the vertical axis, a constant lag implies a horizontal line. The time-lag version of Figure 7 is shown in Figure 9. Only the lines representing similar sequences are shown, and the grayscale has been reversed, so that similarity is indicated by black lines.', metadata={'page': 9, 'source': 'documents/pdf/Music_Structure_Analysis_from_Acoustic_Signals.pdf'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How to go from a time/time similarity matrix to a time/lag surface matrix ?\"\n",
    "matching_docs = db.similarity_search(query)\n",
    "\n",
    "matching_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plug LLM to Chroma DB through langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_KEY = os.environ.get('MISTRAL_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(model=\"mistral-small\", temperature=0, mistral_api_key=MISTRAL_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the answer from the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas/.pyenv/versions/3.10.6/envs/knowledge_QA/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Figure 8. Similarity matrix using spectral features from the bridge of \"Day Tripper\" by the Beatles. The Time-Lag Matrix When the goal is to find repeating sequence patterns, it is sometimes simpler to change coordinate systems so that patterns appear as horizontal or vertical lines. The time-lag matrix r is defined by:   r(t, l) = S(t, t−l), where t−l ≥ 0 (8) Thus, if there is repetition, there will be a sequence of similar frames with a constant lag. Since lag is represented by the vertical axis, a constant lag implies a horizontal line. The time-lag version of Figure 7 is shown in Figure 9. Only the lines representing similar sequences are shown, and the grayscale has been reversed, so that similarity is indicated by black lines.\n",
      "\n",
      "the Time-Lag Matrix If nearly constant tempo can be assumed, the alignment path is highly constrained and the alignment path approach may not work well. Taking advantage of the fact that similar sequences are represented by\n",
      "\n",
      "- 12 - are above a threshold. To allow for slight changes in tempo, which results in lines that are not perfectly horizontal, neighbor cells above and below are also considered. Lu, Wang, and Zhang (Lu, Wang, and Zhang 2004) suggest erosion and dilation operations on the lag matrix to enhance and detect significant similar sequences. Dannenberg and Hu (2003) use a discrete algorithm to find similar sequences and is based on the idea that a path from cell to cell through the similarity matrix specifies an alignment between two subsequences of the feature vectors. If the path goes through S(i, j), then vector i is aligned with j. This suggests using a dynamic time warping (DTW) algorithm (Rabiner and Juang 1993), and the actual algorithm is related to DTW.  The goal is to find alignment paths that maximize the average similarity of the aligned features. A partial or complete path P is defined as a set of pairs of locations and is rated by the average similarity along the path:\n",
      "\n",
      "pair of vectors calculated in step one. We place the results in a\n",
      "similarity matrix that has lines of constant lag oriented along the\n",
      "diagonals of the matrix. Thus, an extended region of similaritybetween two portions of a song will show up as an extended areaof high correlation along one of the diagonals. One example ofsuch a similarity matrix is shown in Figure 1.\n",
      "3.4. Correlation Filtering\n",
      "In the next step, we ﬁlter along the diagonals of the similarity ma-\n",
      "trix to compute similarity between extended regions of the song.The size of these regions is dependent upon the length of the ﬁlter’simpulse response. We use a uniform moving average ﬁlter where\n",
      "the length of the impulse response is left as a design parameter of\n",
      "the system. The ﬁltering results are placed in a restructured time-lag matrix, in which the lines of constant lag are oriented along the\n",
      "16  \n",
      "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics 2001\n",
      "Human: How to go from a time/time similarity matrix to a time/lag surface matrix ?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To go from a time/time similarity matrix to a time/lag surface matrix, you can use the time-lag matrix concept. The time-lag matrix, as defined by r(t, l) = S(t, t−l), where t−l ≥ 0, represents similarity in a coordinate system where repetition appears as horizontal lines. This is because lag is represented by the vertical axis, and a constant lag implies a horizontal line.\\n\\nIn the context of the given text, the time-lag matrix is used to find alignment paths that maximize the average similarity of the aligned features. To create a time/lag surface matrix from a time/time similarity matrix, you would need to transform the coordinate system of the similarity matrix to have lines of constant lag oriented along the diagonals. This would result in an extended area of high correlation along one of the diagonals indicating an extended region of similarity between two portions of a song.\\n\\nAfter this transformation, you can filter along the diagonals of the similarity matrix to compute similarity between extended regions of the song. The size of these regions is dependent upon the length of the filter's impulse response. The filtered results are then placed in a restructured time-lag matrix, in which the lines of constant lag are oriented along the diagonals. This new matrix is the time/lag surface matrix.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_qa_chain(llm, chain_type=\"stuff\",verbose=True)\n",
    "\n",
    "query = \"How to go from a time/time similarity matrix to a time/lag surface matrix ?\"\n",
    "matching_docs = db.similarity_search(query)\n",
    "answer =  chain.run(input_documents=matching_docs, question=query)\n",
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To go from a time/time similarity matrix to a time/lag surface matrix, you can use the time-lag matrix concept. The time-lag matrix, as defined by r(t, l) = S(t, t−l), where t−l ≥ 0, represents similarity in a coordinate system where repetition appears as horizontal lines. This is because lag is represented by the vertical axis, and a constant lag implies a horizontal line.\\n\\nIn the context of the given text, the time-lag matrix is used to find alignment paths that maximize the average similarity of the aligned features. To create a time/lag surface matrix from a time/time similarity matrix, you would need to transform the coordinate system of the similarity matrix to have lines of constant lag oriented along the diagonals. This would result in an extended area of high correlation along one of the diagonals indicating an extended region of similarity between two portions of a song.\\n\\nAfter this transformation, you can filter along the diagonals of the similarity matrix to compute similarity between extended regions of the song. The size of these regions is dependent upon the length of the filter's impulse response. The filtered results are then placed in a restructured time-lag matrix, in which the lines of constant lag are oriented along the diagonals. This new matrix is the time/lag surface matrix.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "retrieval_chain = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "retrieval_chain.run(query)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge_QA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
